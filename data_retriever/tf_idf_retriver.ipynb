{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic TF-IDF approach and the pandas library for data manipulation:\n",
    "# which will search for the top 5 most similar articles to a given query.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv('articles.csv')\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['AB'].fillna(''))\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, tfidf_matrix, data):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "    document_scores = list(enumerate(cosine_similarities))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        print(f\"Title: {data['TI'].iloc[idx]}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "search(\"IQ scores\", tfidf_matrix, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling\n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Use English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Modify the preprocessing step to remove stopwords\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "df['AB'] = df['AB'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# SpellChecker initialization\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(query):\n",
    "    # Tokenize the query\n",
    "    tokens = query.split()\n",
    "\n",
    "    # Correct misspelled words using pyspellchecker\n",
    "    corrected_tokens = [spell.correction(token) for token in tokens]\n",
    "\n",
    "    # Join the corrected tokens back into a corrected query\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    # Tokenize the query\n",
    "    query_tokens = query.lower().split()\n",
    "\n",
    "    # Use regex to split the abstract into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "\n",
    "    # Find the first sentence containing any of the query keywords\n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5):\n",
    "    # Correct misspellings in the query\n",
    "    corrected_query = correct_spelling(query)\n",
    "\n",
    "    # Vectorize the corrected query using the same TF-IDF vectorizer\n",
    "    query_vector = vectorizer.transform([corrected_query])\n",
    "\n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "\n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "\n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling using TF-IDF vectorizer for character-level embeddings\n",
    "# as well as enriching the search by adding synonyms for the nouns \n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "\n",
    "# # Load spaCy English language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download NLTK resources\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# SpellChecker initialization\n",
    "spell = SpellChecker()\n",
    "\n",
    "# NLTK WordNet synonym extraction\n",
    "def get_synonyms(word, pos=None):\n",
    "    # # Map spaCy POS tags to WordNet POS tags\n",
    "    # pos_mapping = {'NOUN': 'n', 'PROPN': 'n', 'VERB': 'v'}\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    pos_mapping = {'NN': 'n', 'VB': 'v'}\n",
    "    pos_tag = pos_mapping.get(pos, 'n') if pos else None\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos_tag):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to replace words in a sentence with their synonyms\n",
    "def replace_with_synonyms(sentence, pos_tags, max_synonyms=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        # Check if the word has a corresponding POS tag\n",
    "        pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "        # If the word has a specific POS tag (e.g., noun), get synonyms\n",
    "        if pos_tag_word and pos_tag_word.startswith(('NN')):\n",
    "            corrected_word = spell.correction(word)\n",
    "            synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])  # Use the first two characters of the tag\n",
    "            print(synonyms)\n",
    "            if synonyms:\n",
    "                # Replace the word with up to max_synonyms synonyms\n",
    "                tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# # Function to replace words in a sentence with their synonyms\n",
    "# def replace_with_synonyms(sentence, max_synonyms=1):\n",
    "#     tokens = word_tokenize(sentence)\n",
    "    \n",
    "#     # Use spaCy for part-of-speech tagging\n",
    "#     pos_tags = [(token.text, token.pos_) for token in nlp(sentence)]\n",
    "    \n",
    "#     for i in range(len(tokens)):\n",
    "#         word = tokens[i]\n",
    "#         # Check if the word has a corresponding POS tag\n",
    "#         pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "#         # If the word has a specific POS tag (e.g., noun), get synonyms\n",
    "#         if pos_tag_word in ['NOUN']:\n",
    "#             corrected_word = spell.correction(word)\n",
    "#             synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])\n",
    "#             print(synonyms)\n",
    "#             if synonyms:\n",
    "#                 # Replace the word with up to max_synonyms synonyms\n",
    "#                 tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "def correct_spelling(query):\n",
    "    # Tokenize the query\n",
    "    tokens = query.split()\n",
    "\n",
    "    # Correct misspelled words using pyspellchecker\n",
    "    corrected_tokens = [spell.correction(token) for token in tokens]\n",
    "\n",
    "    # Join the corrected tokens back into a corrected query\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    # Tokenize the query\n",
    "    query_tokens = query.lower().split()\n",
    "\n",
    "    # Use regex to split the abstract into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "\n",
    "    # Find the first sentence containing any of the query keywords\n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "\n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5, use_synonyms=False, max_synonyms=1):\n",
    "    # Correct misspellings in the query\n",
    "    corrected_query = correct_spelling(query)\n",
    "\n",
    "    # Optionally replace words with synonyms\n",
    "    if use_synonyms:\n",
    "        # Perform part-of-speech tagging\n",
    "        pos_tags = pos_tag(word_tokenize(corrected_query))\n",
    "        enriched_query = replace_with_synonyms(corrected_query, pos_tags, max_synonyms=max_synonyms)\n",
    "    else:\n",
    "        enriched_query = corrected_query\n",
    "\n",
    "    # Vectorize the enriched query using the same TF-IDF vectorizer\n",
    "    query_vector = vectorizer.transform([enriched_query])\n",
    "\n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "\n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "\n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "use_synonyms = True  # Set this flag to control whether to use synonyms or not\n",
    "max_synonyms = 2  # Set the maximum number of synonyms to use for each word\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=use_synonyms, max_synonyms=max_synonyms)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using synonyms but with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without using synonyms \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# using synonyms and misspelling\n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=True)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling using TF-IDF vectorizer for character-level embeddings\n",
    "# and edit distance metric-based approach using the Levenshtein distance algorithm\n",
    "# as well as enriching the search by adding synonyms for the nouns \n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import Levenshtein as lev\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# NLTK WordNet synonym extraction\n",
    "def get_synonyms(word, pos=None):\n",
    "    pos_mapping = {'NN': 'n', 'VB': 'v'}\n",
    "    pos_tag = pos_mapping.get(pos, 'n') if pos else None\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos_tag):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to replace words in a sentence with their synonyms\n",
    "def replace_with_synonyms(sentence, pos_tags, max_synonyms=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "        if pos_tag_word and pos_tag_word.startswith(('NN')):\n",
    "            corrected_word = spell.correction(word)\n",
    "            synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])\n",
    "            if synonyms:\n",
    "                tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "    print(' '.join(tokens))\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def correct_spelling_edit_distance(query):\n",
    "    tokens = query.split()\n",
    "    corrected_tokens = [correct_with_edit_distance(token) for token in tokens]\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "    return corrected_query\n",
    "\n",
    "def correct_with_edit_distance(token):\n",
    "    # Get candidate corrections within a maximum edit distance\n",
    "    candidates = [word for word in vocabulary if lev.distance(token, word) <= max_edit_distance]\n",
    "    \n",
    "    # Choose the candidate with the minimum edit distance\n",
    "    corrected_token = min(candidates, key=lambda x: lev.distance(token, x))\n",
    "    \n",
    "    return corrected_token\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    query_tokens = query.lower().split()\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5, use_synonyms=False, max_synonyms=1):\n",
    "    corrected_query = correct_spelling_edit_distance(query)\n",
    "    \n",
    "    if use_synonyms:\n",
    "        pos_tags = pos_tag(word_tokenize(corrected_query))\n",
    "        enriched_query = replace_with_synonyms(corrected_query, pos_tags, max_synonyms=max_synonyms)\n",
    "    else:\n",
    "        enriched_query = corrected_query\n",
    "    \n",
    "    query_vector = vectorizer.transform([enriched_query])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "    \n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "use_synonyms = True\n",
    "max_synonyms = 2\n",
    "max_edit_distance = 2  # Set the maximum edit distance for the spell-checking\n",
    "spell = SpellChecker(distance=max_edit_distance)\n",
    "vocabulary = set(df['AB'].str.cat(sep=' ').lower().split())\n",
    "\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=use_synonyms, max_synonyms=max_synonyms)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using synonyms but with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without using synonyms \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# using synonyms and misspelling\n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=True)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
