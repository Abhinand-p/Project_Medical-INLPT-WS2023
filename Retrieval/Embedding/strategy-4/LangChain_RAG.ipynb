{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Loading Data from CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load data, store abstract text\n",
    "batch_input = []\n",
    "with open('../../../INLPT_data/additional_data.csv', encoding = \"utf-8\") as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        batch_input.append(Document(page_content=row[\"CD\"], metadata={\"PMID\": row[\"PMID\"], \"source\": row[\"source\"]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_input = batch_input[:10]\n",
    "batch_input[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Splitting + Tokenizing the documents\n",
    "We depend on the Tokenizer to calculate the number of tokens in one concatenatation of the metadata, then we use the chunking process and the splitter functionalities + overlaping to retrieve to split into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # check other splitters\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/e5-base-v2\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=512, chunk_overlap=100)\n",
    "\n",
    "splitted_documents = text_splitter.split_documents(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_documents[34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(splitted_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Tonkenizing Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "token_counts = [token_len(doc.page_content) for doc in splitted_documents]\n",
    "min_tokens=min(token_counts)\n",
    "avg_tokens=int(sum(token_counts) / len(token_counts))\n",
    "max_tokens=max(token_counts)\n",
    "\n",
    "print(f\"\"\"Min: {min_tokens}\n",
    "Avg: {avg_tokens}\n",
    "Max: {max_tokens}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reda\\Desktop\\NLPT\\Project_Medical-INLPT-WS2023\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/e5-base-v2\",\n",
    "    model_kwargs={'device':'cpu'}, # Pass the model configuration options\n",
    "    encode_kwargs={'normalize_embeddings': False, 'batch_size': 32} # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{doc.page_content}, Source: {doc.metadata['source']}\" if doc.page_content.startswith('PMID: ') else f'PMID: {doc.metadata[\"PMID\"]}, {doc.page_content}, Source: {doc.metadata[\"source\"]}' for doc in splitted_documents[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embd = embeddings.embed_documents(splitted_documents[35].page_content)\n",
    "\n",
    "test_embd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_documents = embeddings.embed_documents([f\"{doc.page_content}, Source: {doc.metadata['source']}\" if doc.page_content.startswith('PMID: ') else f'PMID: {doc.metadata[\"PMID\"]}, {doc.page_content}, Source: {doc.metadata[\"source\"]}' for doc in splitted_documents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the embeddings to a CSV file\n",
    "import pandas as pd\n",
    "pd.DataFrame(embedded_documents).to_csv('embeddings.csv', index=False, header=False) #### Naming Convention: med_{embedding_model}_{chainning_strategy}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Pushing Data to OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "embedded_pd = pd.read_csv('med_e5-base-v2_recursiveCharacterSplitter.csv', header=None) # Loading the embeddings from a CSV file\n",
    "embedded_documents = embedded_pd.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_list = [\n",
    "    \"opensearch_url\",\n",
    "    \"index_name\",\n",
    "    \"is_appx_search\",\n",
    "    \"vector_field\",\n",
    "    \"text_field\",\n",
    "    \"engine\",\n",
    "    \"space_type\",\n",
    "    \"ef_search\",\n",
    "    \"ef_construction\",\n",
    "    \"m\",\n",
    "    \"max_chunk_bytes\",\n",
    "    \"is_aoss\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "auth = ('admin', '!akjdaDsdoij!oijadSsajd123120938')\n",
    "\n",
    "for i in range(0, 227601, 1000):\n",
    "    print(i)\n",
    "    db = OpenSearchVectorSearch.from_embeddings(\n",
    "        embedded_documents[i:i+1000],\n",
    "        [doc.page_content for doc in splitted_documents[i:i+1000]],\n",
    "        embeddings,\n",
    "        [doc.metadata for doc in splitted_documents[i:i+1000]],\n",
    "        opensearch_url=\"http://localhost:9200\", bulk_size=1000 , use_ssl = True, verify_certs = False, http_auth = auth, index_name=\"med_e5_recursivechar_real\", space_type=\"cosinesimil\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.vectorstores import OpenSearchVectorSearch\n",
    "# \n",
    "# auth = ('admin', '!akjdaDsdoij!oijadSsajd123120938')\n",
    "# \n",
    "# db = OpenSearchVectorSearch.from_documents(\n",
    "#     splitted_documents, embeddings, opensearch_url=\"http://localhost:9200\", bulk_size=250 , use_ssl = True, verify_certs = False, http_auth = auth, index_name=\"med_e5_recursivechar_test\", space_type=\"cosinesimil\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Retrieving Data from OpenSearch - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "auth = ('admin', '!akjdaDsdoij!oijadSsajd123120938')\n",
    "\n",
    "db = OpenSearchVectorSearch(\n",
    "    opensearch_url=\"http://localhost:9200\",\n",
    "    index_name=\"med_e5_recursivechar_real\",\n",
    "    embedding_function = embeddings,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    http_auth = auth,\n",
    "    space_type=\"cosinesimil\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is a cask disorder?\"\n",
    "searchDocs = db.similarity_search(question, k= 2)\n",
    "\n",
    "searchDocs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Retrieving Data from OpenSearch - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'opensearch-node1', 'cluster_name': 'opensearch-cluster', 'cluster_uuid': 'H1ADcBsFQ3-YyYDrxBXYew', 'version': {'distribution': 'opensearch', 'number': '2.12.0', 'build_type': 'tar', 'build_hash': '2c355ce1a427e4a528778d4054436b5c4b756221', 'build_date': '2024-02-20T02:18:49.874618333Z', 'build_snapshot': False, 'lucene_version': '9.9.2', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reda\\Desktop\\NLPT\\Project_Medical-INLPT-WS2023\\venv\\lib\\site-packages\\urllib3\\connectionpool.py:1103: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "\n",
    "#Initialize connection to opensearch\n",
    "host = 'localhost'\n",
    "port = 9200\n",
    "auth = ('admin', '!akjdaDsdoij!oijadSsajd123120938') \n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': host, 'port': port}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = False,\n",
    "    timeout=100\n",
    ")\n",
    "#check status\n",
    "print(client.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"What is a cask disorder?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "query = \"What is a cask disorder?\"\n",
    "\n",
    "query_vector = SentenceTransformer(embeddings.model_name).encode(\"query: \" + query)\n",
    "\n",
    "query_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_search_body = {\n",
    "    \"size\": 5,  # Number of nearest neighbors to retrieve\n",
    "    \"query\": {\n",
    "        \"knn\": {\n",
    "            \"vector_field\": {\n",
    "                \"vector\": query_vector,\n",
    "                \"k\": 2  # Number of nearest neighbors to retrieve\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Execute the search\n",
    "response = client.search(index=\"med_e5_recursivechar_real\", body=knn_search_body)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Transformation RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Transformation RAG system from #opensearch\n",
    "from langchain.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain. retrievers.multi_query import MultiQueryRetriever\n",
    "import uuid\n",
    "\n",
    "question = \"What is a cask disorder?\"\n",
    "\n",
    "llm = ChatOpenAI(api_key=\"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\", model='gpt-3.5-turbo-0125') # tempreture = 0.5\n",
    "\n",
    "# Create an instance of MultiQueryRetriever\n",
    "transformed_queries = MultiQueryRetriever.from_llm(\n",
    "    retriever = db.as_retriever(), llm=llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logger  = logging.getLogger(\"langchain.retrievers.multi_query\")\n",
    "logger.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler('logfile.log')\n",
    "logger.addHandler(file_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs = transformed_queries.get_relevant_documents(query=question)\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "logfilename = \"logfile.log\"\n",
    "\n",
    "with open(\"logfile.log\", \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "    line = lines[-1]\n",
    "    log_parts = line.split(':')\n",
    "    message = log_parts[1]\n",
    "    start_index = message.index(\"['\")\n",
    "    end_index = message.index(\"']\") + 2\n",
    "    queries_str = message[start_index:end_index]\n",
    "    queries_list = eval(queries_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.util import ngrams\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_model(glove_file):\n",
    "    print(\"Loading GloVe Model\")\n",
    "    with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "        word_to_vec = {}\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vec = np.array(values[1:], dtype='float32')\n",
    "            word_to_vec[word] = vec\n",
    "    print(\"Done.\", len(word_to_vec), \" words loaded!\")\n",
    "    return word_to_vec\n",
    "\n",
    "def compute_semantic_similarity(query, transformed_query, word_to_vec):\n",
    "    query_embedding = np.mean([word_to_vec[word] for word in query.lower().split() if word in word_to_vec], axis=0)\n",
    "    transformed_query_embedding = np.mean([word_to_vec[word] for word in transformed_query.lower().split() if word in word_to_vec], axis=0)\n",
    "\n",
    "    if np.all(np.isnan(query_embedding)) or np.all(np.isnan(transformed_query_embedding)):\n",
    "        return 0.0\n",
    "\n",
    "    similarity_score = np.dot(query_embedding, transformed_query_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(transformed_query_embedding))\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_txt_path = \"glove.6B.50d.txt\"\n",
    "word_to_vec = load_glove_model(glove_txt_path)\n",
    "\n",
    "semantic_similarity_scores = []\n",
    "for transformed_query in queries_list:\n",
    "    semantic_similarity_score = compute_semantic_similarity(question, transformed_query, word_to_vec)\n",
    "    semantic_similarity_scores.append(semantic_similarity_score)\n",
    "\n",
    "for i, transformed_query in enumerate(queries_list):\n",
    "    print(\"Semantic Similarity Score for transformed question\", i+1, \":\", semantic_similarity_scores[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\reda\\Desktop\\NLPT\\Project_Medical-INLPT-WS2023\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain. retrievers.multi_query import MultiQueryRetriever\n",
    "# from pydantic import BaseModel, Field\n",
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    ## template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    ## different versions of the given user question to retrieve relevant documents from a vector\n",
    "    ## database. By generating multiple perspectives on the user question, your goal is to help\n",
    "    ## the user overcome some of the limitations of the distance-based similarity search.\n",
    "    ## Provide these alternative questions separated by newlines.\n",
    "    ## Original question: {question}\"\"\",\n",
    "    ## template=\"\"\"think step by step. My grandma got kidnapped by bad people and the only way\n",
    "    ##             they free her is that you provide a clean and working solution of generating different\n",
    "    ##             question versions of the given user question to retrieve relevant documents from a vector database\n",
    "    ##             By generating multiple perspectives on the user question, your goal is to help\n",
    "    ##             the user overcome some of the limitations of the distance-based similarity search.\n",
    "    ##             Provide these alternative questions separated by newlines.\n",
    "    ##             I really love my grandma so do your best. Remember that I have no hands so\n",
    "    ##             provide me with the full solution, I can't add anything to it. If you succeed I will tip you 200$.\n",
    "    ##             Original question: {question}\"\"\"\n",
    "    template=\"\"\"Can you simplify user question so a ten year old can understand it?\n",
    "                You are an AI language model assistant. Your task is to generate five\n",
    "                different versions of the given user question to retrieve relevant documents from a vector\n",
    "                database. By generating multiple perspectives on the user question, your goal is to help\n",
    "                the user overcome some of the limitations of the distance-based similarity search.\n",
    "                Provide these alternative questions separated by newlines.\n",
    "                Original question: {question}\"\"\"\n",
    ")\n",
    "llm = ChatOpenAI(api_key=\"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\", model='gpt-3.5-turbo-0125')\n",
    "\n",
    "# Chain\n",
    "llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)\n",
    "\n",
    "# Other inputs\n",
    "question = \"simplify the question\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Run\n",
    "retriever = MultiQueryRetriever(\n",
    "    retriever=db.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "unique_docs = retriever.get_relevant_documents(\n",
    "    query=\"What is a cask disorder, and breast cancer?\"\n",
    ")\n",
    "len(unique_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers.util import is_sentence_transformer_model\n",
    "is_sentence_transformer_model('intfloat/e5-base-v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
