{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57560\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "file_path = '../../../articles.csv'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    csv_reader = list(csv.reader(file))\n",
    "    csv_reader.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_items = random.sample(csv_reader, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['34821396', 'Deep Learning Applied to White Light and Narrow Band Imaging Videolaryngoscopy: Toward Real-Time Laryngeal Cancer Detection.', 'OBJECTIVES: To assess a new application of artificial intelligence for real-time detection of laryngeal squamous cell carcinoma (LSCC) in both white light (WL)and narrow-band imaging (NBI) videolaryngoscopies based on the You-Only-Look-Once(YOLO) deep learning convolutional neural network (CNN). STUDY DESIGN:Experimental study with retrospective data. METHODS: Recorded videos of LSCC wereretrospectively collected from in-office transnasal videoendoscopies andintraoperative rigid endoscopies. LSCC videoframes were extracted for training,validation, and testing of various YOLO models. Different techniques were used toenhance the image analysis: contrast limited adaptive histogram equalization,data augmentation techniques, and test time augmentation (TTA). Thebest-performing model was used to assess the automatic detection of LSCC in sixvideolaryngoscopies. RESULTS: Two hundred and nineteen patients wereretrospectively enrolled. A total of 624 LSCC videoframes were extracted. TheYOLO models were trained after random distribution of images into a training set(82.6%), validation set (8.2%), and testing set (9.2%). Among the various models,the ensemble algorithm (YOLOv5s with YOLOv5m-TTA) achieved the best LSCCdetection results, with performance metrics in par with the results reported byother state-of-the-art detection models: 0.66 Precision (positive predictedvalue), 0.62 Recall (sensitivity), and 0.63 mean Average Precision at 0.5intersection over union. Tests on the six videolaryngoscopies demonstrated anaverage computation time per videoframe of 0.026 seconds. Three demonstrationvideos are provided. CONCLUSION: This study identified a suitable CNN model forLSCC detection in WL and NBI videolaryngoscopies. Detection performances arehighly promising. The limited complexity and quick computational times for LSCCdetection make this model ideal for real-time processing. LEVEL OF EVIDENCE: 3Laryngoscope, 132:1798-1806, 2022.', '', 'Azam, Muhammad Adeel|Sampieri, Claudio|Ioppi, Alessandro|Africano, Stefano|Vallin, Alberto|Mocellin, Davide|Fragale, Marco|Guastini, Luca|Moccia, Sara|Piazza, Cesare|Mattos, Leonardo S|Peretti, Giorgio', '', '2022 Sep', 'NOTNLM', 'Larynx cancer|computer-assisted image interpretation|deep learning|narrow band imaging|videolaryngoscopy', 'NLM', '20220819', '20221015', 'The Laryngoscope', 'Artificial Intelligence|*Deep Learning|Humans|*Laryngeal Neoplasms/diagnostic imaging|*Laryngoscopes|Laryngoscopy|Narrow Band Imaging/methods|Retrospective Studies', '']]\n"
     ]
    }
   ],
   "source": [
    "print(random_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Testset Generatation only supports LlamaindexDocuments or LangchainDocuments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 29\u001b[0m\n\u001b[0;32m     17\u001b[0m testset_distribution \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     19\u001b[0m }\n\u001b[0;32m     22\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m TestsetGenerator(\n\u001b[0;32m     23\u001b[0m     generator_llm\u001b[38;5;241m=\u001b[39mgenerator_llm,\n\u001b[0;32m     24\u001b[0m     critic_llm\u001b[38;5;241m=\u001b[39mcritic_llm,\n\u001b[0;32m     25\u001b[0m     embeddings_model\u001b[38;5;241m=\u001b[39membeddings_model,\n\u001b[0;32m     26\u001b[0m     testset_distribution\u001b[38;5;241m=\u001b[39mtestset_distribution,\n\u001b[0;32m     27\u001b[0m )\n\u001b[1;32m---> 29\u001b[0m testset \u001b[38;5;241m=\u001b[39m \u001b[43mtest_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ragas\\testset\\testset_generator.py:300\u001b[0m, in \u001b[0;36mTestsetGenerator.generate\u001b[1;34m(self, documents, test_size)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[0;32m    295\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    296\u001b[0m     documents: t\u001b[38;5;241m.\u001b[39mList[LlamaindexDocument] \u001b[38;5;241m|\u001b[39m t\u001b[38;5;241m.\u001b[39mList[LangchainDocument],\n\u001b[0;32m    297\u001b[0m     test_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m    298\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TestDataset:\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(documents[\u001b[38;5;241m0\u001b[39m], (LlamaindexDocument, LangchainDocument)):\n\u001b[1;32m--> 300\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    301\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTestset Generatation only supports LlamaindexDocuments or LangchainDocuments\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    302\u001b[0m         )\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(documents[\u001b[38;5;241m0\u001b[39m], LangchainDocument):\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;66;03m# cast to LangchainDocument since its the only case here\u001b[39;00m\n\u001b[0;32m    306\u001b[0m         documents \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mcast(t\u001b[38;5;241m.\u001b[39mList[LangchainDocument], documents)\n",
      "\u001b[1;31mValueError\u001b[0m: Testset Generatation only supports LlamaindexDocuments or LangchainDocuments"
     ]
    }
   ],
   "source": [
    "from ragas.testset import TestsetGenerator\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from ragas.llms import LangchainLLM\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema.document import Document\n",
    "documents = Document(page_content=\"hello this is ceren and i am 21 years old. I work in Douglas in Heidelberg and i have 2 Sisters, Eda and Mery\", metadata={\"source\": \"local\"})\n",
    "#documents =  Document(page_content=\"hello this is ceren and i am 21 years old. I work in Douglas in Heidelberg and i have 2 Sisters, Eda and Mery\")\n",
    "\n",
    "# Add custom llms and embeddings\n",
    "generator_llm = LangchainLLM(llm=ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "critic_llm = LangchainLLM(llm=ChatOpenAI(model=\"gpt-3.5-turbo\"))\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Change resulting question type distribution\n",
    "testset_distribution = {\n",
    "    \"simple\": 1,\n",
    "}\n",
    "\n",
    "\n",
    "test_generator = TestsetGenerator(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings_model=embeddings_model,\n",
    "    testset_distribution=testset_distribution,\n",
    ")\n",
    "\n",
    "testset = test_generator.generate(list(documents), test_size=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
