{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO RUN THIS CELL AS THE DATA IS ALREADY COLLECTED AND SAVE UNDER articles.csv\n",
    "\n",
    "# Data Crawler that works exactly like a human and go one by one through the articles and save the abstracts and references in XML format\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://pubmed.ncbi.nlm.nih.gov/?term=intelligence+%5BTitle%2Fabstract%5D&filter=simsearch1.fha&filter=years.2013-2023&sort=date&size=200\"\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "driver.get(url)\n",
    "\n",
    "# Click the first article to start\n",
    "driver.find_element(By.XPATH, \"//a[@data-ga-action=1]\").click()\n",
    "total_articles = driver.find_element(By.XPATH, \"//*[@id='adjacent-navigation']/div[2]/a/span[1]/span[2]\").text\n",
    "\n",
    "# Find the number of total articles\n",
    "total_articles = re.sub(r\"[^3-9]\",'', total_articles)\n",
    "\n",
    "# Specify the chunk size\n",
    "chunk_size = 1000\n",
    "\n",
    "# Create empty lists to store data\n",
    "titles = []\n",
    "authors = []\n",
    "abstracts = []\n",
    "references_list = []\n",
    "not_found_pages = []\n",
    "\n",
    "for page in tqdm(range(int(total_articles))):\n",
    "    # Extract title of the article\n",
    "    try:\n",
    "        title = driver.find_element(By.CLASS_NAME, \"heading-title\")\n",
    "        if title.is_displayed():\n",
    "            title = title.text\n",
    "    except NoSuchElementException:\n",
    "        title = ''\n",
    "        pass\n",
    "\n",
    "    # Extract autors of the article\n",
    "    try:\n",
    "        authors_elements = driver.find_elements(By.CLASS_NAME, \"full-name\")\n",
    "        author_list = []\n",
    "        if len(authors_elements) > 0:\n",
    "            for author in authors_elements:\n",
    "                author_list.append(author.text)\n",
    "    except NoSuchElementException:\n",
    "        author_list.append('')\n",
    "        pass\n",
    "\n",
    "    # Extract abstract of the article\n",
    "    try:\n",
    "        abstract = driver.find_element(By.ID, \"eng-abstract\")\n",
    "        if abstract.is_displayed():\n",
    "            abstract = abstract.text\n",
    "    except NoSuchElementException:\n",
    "        abstract = ''\n",
    "        pass\n",
    "\n",
    "    # Check and extract if there is reference or are more references \n",
    "    try:\n",
    "        reference = driver.find_element(By.ID, \"references\")\n",
    "        show_all_element = driver.find_element(By.CLASS_NAME, \"show-all\")\n",
    "        if show_all_element.is_displayed():\n",
    "            show_all_element.click()\n",
    "        if reference.is_displayed():\n",
    "            references = driver.find_element(By.CLASS_NAME, \"references-list\").text\n",
    "    except NoSuchElementException:\n",
    "        references = ''\n",
    "        pass\n",
    "\n",
    "    # Append data to lists\n",
    "    titles.append(title)\n",
    "    authors.append(author_list)\n",
    "    abstracts.append(abstract)\n",
    "    references_list.append(references)\n",
    "\n",
    "    if (page + 1) % chunk_size == 0 or page + 1 == int(total_articles):\n",
    "        # Create a DataFrame\n",
    "        data = {\n",
    "                    'Title': pd.Series(titles),\n",
    "                    'Authors': pd.Series(authors),\n",
    "                    'Abstracts': pd.Series(abstracts),\n",
    "                    'References': pd.Series(references_list)\n",
    "                }\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        chunk_number = (page + 1) // chunk_size\n",
    "        csv_filename = f'pubmed_data_chunk_{chunk_number}.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        # Clear lists for the next chunk\n",
    "        titles = []\n",
    "        authors = []\n",
    "        abstracts = []\n",
    "        references_list = []\n",
    "    \n",
    "    # Navigate to the next article\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, \"//div[@class='next side-link visible']\")\n",
    "        if next_page.is_displayed():\n",
    "            next_page.click()\n",
    "    except NoSuchElementException:\n",
    "        not_found_pages.append(page)\n",
    "        print(f\"{page = } not found!\")\n",
    "        pass\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing to have a CSV file with the following columns and respective data in each row:\n",
    "# which will be filtered later to have just useful columns and non dupplicated PMID\n",
    "\n",
    "\"\"\"\n",
    "df shape before cleaning:(74243, 77)\n",
    "df shape after cleaning:(57560, 15)\n",
    "\n",
    "Index(['PMID', 'STAT', 'DRDT', 'CTDT', 'PB', 'DP', 'TI', 'BTI', 'AB', 'CI',\n",
    "       'FED', 'ED', 'FAU', 'AU', 'AD', 'LA', 'PT', 'PL', 'OTO', 'OT', 'EDAT',\n",
    "       'CRDT', 'AID', 'OWN', 'DCOM', 'LR', 'IS', 'VI', 'IP', 'PG', 'LID',\n",
    "       'DEP', 'TA', 'JT', 'JID', 'SB', 'MH', 'MHDA', 'PHST', 'PST', 'SO', 'GR',\n",
    "       'PMC', 'MID', 'COIS', 'TT', 'RN', 'OID', 'SI', 'ISBN', 'CTI', 'CN',\n",
    "       'FIR', 'IR', 'AUID', 'EIN', 'CIN', 'PS', 'FPS', 'CON', 'UOF', 'UIN',\n",
    "       'RIN', 'IRAD', 'EFR', 'OAB', 'OABL', 'PMCR', 'CP', 'ECI', 'DRIN', 'RF',\n",
    "       'EN', 'ROF', 'RPI', 'RPF', 'DDIN'],\n",
    "      dtype='object')\n",
    "\n",
    "\n",
    "Guide to the abbreviations:\n",
    "\n",
    "    PMID: PubMed IDentifier - Unique identifier for a PubMed record.\n",
    "\n",
    "    TI: Title - The title of the article.\n",
    "\n",
    "    AB: Abstract - A brief summary of the article's content.\n",
    "\n",
    "    PB: Publisher - The organization responsible for publishing the article.\n",
    "\n",
    "    FAU: Full Author(s) - Full names of the authors.\n",
    "\n",
    "    FED: Full Editor(s) - Full names of the editors.\n",
    "\n",
    "    DP: Date of Publication - Date when the article was published.\n",
    "\n",
    "    OTO: Other Term Owner - Owner of other terms.\n",
    "\n",
    "    OT: Other Term - Additional terms or keywords associated with the article.\n",
    "\n",
    "    OWN: Owner - Owner of the article.\n",
    "\n",
    "    DCOM: Date Completed - Date when the article was completed.\n",
    "\n",
    "    LR: Last Revision - Last revision date.\n",
    "\n",
    "    JT: Journal Title - Full title of the journal.\n",
    "\n",
    "    MH: MeSH Terms - Medical Subject Headings.\n",
    "\n",
    "    ISBN: International Standard Book Number - ISBN of the article.\n",
    "\n",
    "[Removed]    STAT: Status - Indicates the status of the publication.\n",
    "\n",
    "[Removed]    DRDT: Date Received by Database Transfer - Date when the record was received by the database.\n",
    "\n",
    "[Removed]    CTDT: Current Temporary Date - Current temporary date of the record.\n",
    "\n",
    "[Removed]    BTI: Book Title Indicator - Indicates that the article is part of a book.\n",
    "\n",
    "[Removed]    CI: Copyright Information - Information about the copyright holder.\n",
    "\n",
    "[Removed]    ED: Editor - Abbreviation for the editor.\n",
    "\n",
    "[Removed]    AU: Author - Abbreviation for the author.\n",
    "\n",
    "[Removed]    AD: Author's Affiliation - Affiliation or institution of the author.\n",
    "\n",
    "[Removed]    LA: Language - Language of the article.\n",
    "\n",
    "[Removed]    PT: Publication Type - Type of publication (e.g., Review, Book Chapter).\n",
    "\n",
    "[Removed]    PL: Place of Publication - Location where the article was published.\n",
    "\n",
    "[Removed]    EDAT: Entrez Date - Date the record was added to the Entrez database.\n",
    "\n",
    "[Removed]    CRDT: Create Date - Date the record was created.\n",
    "\n",
    "[Removed]    AID: Article Identifier - Identifier associated with the article.\n",
    "\n",
    "[Removed]    IS: Issue - Issue number of the journal.\n",
    "\n",
    "[Removed]    VI: Volume - Volume number of the journal.\n",
    "\n",
    "[Removed]    IP: Issue Part - Part number of the issue.\n",
    "\n",
    "[Removed]    PG: Page - Page number.\n",
    "\n",
    "[Removed]    LID: Location IDentifier - Identifier for the location of the article.\n",
    "\n",
    "[Removed]    DEP: Date of Electronic Publication - Date of electronic publication.\n",
    "\n",
    "[Removed]    TA: Journal Title (ISO abbreviation) - Title abbreviation of the journal.\n",
    "\n",
    "[Removed]    JID: Journal ID - Identifier for the journal.\n",
    "\n",
    "[Removed]    SB: Subset - Subset designation.\n",
    "\n",
    "[Removed]    MHDA: MeSH Date - MeSH date.\n",
    "\n",
    "[Removed]    PHST: Publication History Status - Publication history status.\n",
    "\n",
    "[Removed]    PST: Publication Status - Publication status.\n",
    "\n",
    "[Removed]    SO: Source - Source of the article.\n",
    "\n",
    "[Removed]    GR: Grant - Grant information.\n",
    "\n",
    "[Removed]    PMC: PubMed Central ID - Identifier for PubMed Central.\n",
    "\n",
    "[Removed]    MID: Manuscript ID - Identifier for the manuscript.\n",
    "\n",
    "[Removed]    COIS: Conflict of Interest Statement - Statement about potential conflicts of interest.\n",
    "\n",
    "[Removed]    TT: Type of Test - Type of test.\n",
    "\n",
    "[Removed]    RN: Registry Number - Registry number.\n",
    "\n",
    "[Removed]    OID: Organization ID - Identifier for the organization.\n",
    "\n",
    "[Removed]    SI: Secondary Source ID - Secondary source identifier.\n",
    "\n",
    "[Removed]    CTI: Current Technology Information - Current technology information.\n",
    "\n",
    "[Removed]    CN: Contract Number - Contract number.\n",
    "\n",
    "[Removed]    FIR: Full Investigator(s) - Full names of the investigators.\n",
    "\n",
    "[Removed]    IR: Investigator - Abbreviation for the investigator.\n",
    "\n",
    "[Removed]    AUID: Author ID - Identifier for the author.\n",
    "\n",
    "[Removed]    EIN: Editor's ID - Identifier for the editor.\n",
    "\n",
    "[Removed]    CIN: Contributor ID - Identifier for the contributor.\n",
    "\n",
    "[Removed]    PS: Personal Name as Subject - Personal name as subject.\n",
    "\n",
    "[Removed]    FPS: Full Personal Name as Subject - Full personal name as subject.\n",
    "\n",
    "[Removed]    CON: Consortium - Consortium information.\n",
    "\n",
    "[Removed]    UOF: Use of Funds - Use of funds information.\n",
    "\n",
    "[Removed]    UIN: Unique Identifier - Unique identifier.\n",
    "\n",
    "[Removed]    RIN: Reviewer ID - Reviewer identifier.\n",
    "\n",
    "[Removed]    IRAD: Investigator Affiliation Department - Investigator affiliation department.\n",
    "\n",
    "[Removed]    EFR: EFS (Endoscopic Frequency Standardization) Factor - EFS factor.\n",
    "\n",
    "[Removed]    OAB: Overall Bank - Overall bank.\n",
    "\n",
    "[Removed]    OABL: Overall Blood - Overall blood.\n",
    "\n",
    "[Removed]    PMCR: PubMed Central Release - PubMed Central release information.\n",
    "\n",
    "[Removed]    CP: Clinical Progress - Clinical progress.\n",
    "\n",
    "[Removed]    ECI: Early Career Investigator - Early career investigator.\n",
    "\n",
    "[Removed]    DRIN: Dual Purpose Experimental Purpose Indicator - Dual-purpose experimental purpose indicator.\n",
    "\n",
    "[Removed]    RF: Release Factor - Release factor.\n",
    "\n",
    "[Removed]    EN: Endorsement - Endorsement.\n",
    "\n",
    "[Removed]    ROF: Reviewer's Office - Reviewer's office.\n",
    "\n",
    "[Removed]    RPI: Reviewer's Position Identifier - Reviewer's position identifier.\n",
    "\n",
    "[Removed]    RPF: Research Performance Factor - Research performance factor.\n",
    "\n",
    "[Removed]    DDIN: Degree-Degree Integration Network - Degree-degree integration network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open('../crawler/articles.txt', 'r', encoding='utf-8') as f:\n",
    "    input_text = f.read()\n",
    "\n",
    "# Split articles based on double quotes\n",
    "articles = re.split(r'\\n\"\\n', input_text.strip())\n",
    "\n",
    "# Define a function to extract data from each article\n",
    "def extract_data(article):\n",
    "    data = {}\n",
    "    current_key = None\n",
    "    current_value = ''\n",
    "\n",
    "    for line in article.split('\\n'):\n",
    "        # matching the key-value pair\n",
    "        match = re.match(r'^([A-Z]{2,4})\\s*- (.+)$', line)\n",
    "\n",
    "        if match:\n",
    "            key, value = match.groups()\n",
    "            if current_key:\n",
    "                # If a key is already set, save the current value\n",
    "                if current_key in data:\n",
    "                    data[current_key] += '|' + current_value\n",
    "                else:\n",
    "                    data[current_key] =  current_value.strip()\n",
    "                current_value = ''  # Reset current value\n",
    "\n",
    "            current_key = key\n",
    "            current_value = value\n",
    "        else:\n",
    "            # If there's no match, append the line to the current value\n",
    "            current_value += '' + line.strip()\n",
    "\n",
    "    # Save the last key-value pair\n",
    "    if current_key:\n",
    "        data[current_key] = current_value.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Extract data from each article\n",
    "article_data_list = [extract_data(article) for article in articles]\n",
    "\n",
    "# Filter out articles without 'AB' key\n",
    "filtered_data_list = [data for data in article_data_list if 'AB' in data]\n",
    "\n",
    "# Create a DataFrame from the filtered data\n",
    "df = pd.DataFrame(filtered_data_list)\n",
    "\n",
    "# Keep only useful columns:  df shape before cleaning:(74243, 77)\n",
    "df = df[['PMID', 'TI', 'AB', 'PB', 'FAU', 'FED', 'DP', 'OTO', 'OT', 'OWN', 'DCOM', 'LR', 'JT', 'MH', 'ISBN']]\n",
    "\n",
    "# Drop duplicates based on the 'PMID' column : df shape after cleaning:(57560, 15)\n",
    "df = df.drop_duplicates(subset='PMID', keep='first')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../crawler/articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic TF-IDF approach and the pandas library for data manipulation:\n",
    "# which will search for the top 5 most similar articles to a given query.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv('articles.csv')\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['AB'].fillna(''))\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, tfidf_matrix, data):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "    document_scores = list(enumerate(cosine_similarities))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        print(f\"Title: {data['TI'].iloc[idx]}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "search(\"IQ scores\", tfidf_matrix, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling\n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Use English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Modify the preprocessing step to remove stopwords\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "df['AB'] = df['AB'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# SpellChecker initialization\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(query):\n",
    "    # Tokenize the query\n",
    "    tokens = query.split()\n",
    "\n",
    "    # Correct misspelled words using pyspellchecker\n",
    "    corrected_tokens = [spell.correction(token) for token in tokens]\n",
    "\n",
    "    # Join the corrected tokens back into a corrected query\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    # Tokenize the query\n",
    "    query_tokens = query.lower().split()\n",
    "\n",
    "    # Use regex to split the abstract into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "\n",
    "    # Find the first sentence containing any of the query keywords\n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5):\n",
    "    # Correct misspellings in the query\n",
    "    corrected_query = correct_spelling(query)\n",
    "\n",
    "    # Vectorize the corrected query using the same TF-IDF vectorizer\n",
    "    query_vector = vectorizer.transform([corrected_query])\n",
    "\n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "\n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "\n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling using TF-IDF vectorizer for character-level embeddings\n",
    "# as well as enriching the search by adding synonyms for the nouns \n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "\n",
    "# # Load spaCy English language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download NLTK resources\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# SpellChecker initialization\n",
    "spell = SpellChecker()\n",
    "\n",
    "# NLTK WordNet synonym extraction\n",
    "def get_synonyms(word, pos=None):\n",
    "    # # Map spaCy POS tags to WordNet POS tags\n",
    "    # pos_mapping = {'NOUN': 'n', 'PROPN': 'n', 'VERB': 'v'}\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    pos_mapping = {'NN': 'n', 'VB': 'v'}\n",
    "    pos_tag = pos_mapping.get(pos, 'n') if pos else None\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos_tag):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to replace words in a sentence with their synonyms\n",
    "def replace_with_synonyms(sentence, pos_tags, max_synonyms=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        # Check if the word has a corresponding POS tag\n",
    "        pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "        # If the word has a specific POS tag (e.g., noun), get synonyms\n",
    "        if pos_tag_word and pos_tag_word.startswith(('NN')):\n",
    "            corrected_word = spell.correction(word)\n",
    "            synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])  # Use the first two characters of the tag\n",
    "            print(synonyms)\n",
    "            if synonyms:\n",
    "                # Replace the word with up to max_synonyms synonyms\n",
    "                tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# # Function to replace words in a sentence with their synonyms\n",
    "# def replace_with_synonyms(sentence, max_synonyms=1):\n",
    "#     tokens = word_tokenize(sentence)\n",
    "    \n",
    "#     # Use spaCy for part-of-speech tagging\n",
    "#     pos_tags = [(token.text, token.pos_) for token in nlp(sentence)]\n",
    "    \n",
    "#     for i in range(len(tokens)):\n",
    "#         word = tokens[i]\n",
    "#         # Check if the word has a corresponding POS tag\n",
    "#         pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "#         # If the word has a specific POS tag (e.g., noun), get synonyms\n",
    "#         if pos_tag_word in ['NOUN']:\n",
    "#             corrected_word = spell.correction(word)\n",
    "#             synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])\n",
    "#             print(synonyms)\n",
    "#             if synonyms:\n",
    "#                 # Replace the word with up to max_synonyms synonyms\n",
    "#                 tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "def correct_spelling(query):\n",
    "    # Tokenize the query\n",
    "    tokens = query.split()\n",
    "\n",
    "    # Correct misspelled words using pyspellchecker\n",
    "    corrected_tokens = [spell.correction(token) for token in tokens]\n",
    "\n",
    "    # Join the corrected tokens back into a corrected query\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    # Tokenize the query\n",
    "    query_tokens = query.lower().split()\n",
    "\n",
    "    # Use regex to split the abstract into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "\n",
    "    # Find the first sentence containing any of the query keywords\n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "\n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5, use_synonyms=False, max_synonyms=1):\n",
    "    # Correct misspellings in the query\n",
    "    corrected_query = correct_spelling(query)\n",
    "\n",
    "    # Optionally replace words with synonyms\n",
    "    if use_synonyms:\n",
    "        # Perform part-of-speech tagging\n",
    "        pos_tags = pos_tag(word_tokenize(corrected_query))\n",
    "        enriched_query = replace_with_synonyms(corrected_query, pos_tags, max_synonyms=max_synonyms)\n",
    "    else:\n",
    "        enriched_query = corrected_query\n",
    "\n",
    "    # Vectorize the enriched query using the same TF-IDF vectorizer\n",
    "    query_vector = vectorizer.transform([enriched_query])\n",
    "\n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "\n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "\n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "use_synonyms = True  # Set this flag to control whether to use synonyms or not\n",
    "max_synonyms = 2  # Set the maximum number of synonyms to use for each word\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=use_synonyms, max_synonyms=max_synonyms)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using synonyms but with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without using synonyms \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# using synonyms and misspelling\n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=True)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling using TF-IDF vectorizer for character-level embeddings\n",
    "# and edit distance metric-based approach using the Levenshtein distance algorithm\n",
    "# as well as enriching the search by adding synonyms for the nouns \n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import Levenshtein as lev\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# NLTK WordNet synonym extraction\n",
    "def get_synonyms(word, pos=None):\n",
    "    pos_mapping = {'NN': 'n', 'VB': 'v'}\n",
    "    pos_tag = pos_mapping.get(pos, 'n') if pos else None\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos_tag):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to replace words in a sentence with their synonyms\n",
    "def replace_with_synonyms(sentence, pos_tags, max_synonyms=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "        if pos_tag_word and pos_tag_word.startswith(('NN')):\n",
    "            corrected_word = spell.correction(word)\n",
    "            synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])\n",
    "            if synonyms:\n",
    "                tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "    print(' '.join(tokens))\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def correct_spelling_edit_distance(query):\n",
    "    tokens = query.split()\n",
    "    corrected_tokens = [correct_with_edit_distance(token) for token in tokens]\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "    return corrected_query\n",
    "\n",
    "def correct_with_edit_distance(token):\n",
    "    # Get candidate corrections within a maximum edit distance\n",
    "    candidates = [word for word in vocabulary if lev.distance(token, word) <= max_edit_distance]\n",
    "    \n",
    "    # Choose the candidate with the minimum edit distance\n",
    "    corrected_token = min(candidates, key=lambda x: lev.distance(token, x))\n",
    "    \n",
    "    return corrected_token\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    query_tokens = query.lower().split()\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5, use_synonyms=False, max_synonyms=1):\n",
    "    corrected_query = correct_spelling_edit_distance(query)\n",
    "    \n",
    "    if use_synonyms:\n",
    "        pos_tags = pos_tag(word_tokenize(corrected_query))\n",
    "        enriched_query = replace_with_synonyms(corrected_query, pos_tags, max_synonyms=max_synonyms)\n",
    "    else:\n",
    "        enriched_query = corrected_query\n",
    "    \n",
    "    query_vector = vectorizer.transform([enriched_query])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "    \n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "use_synonyms = True\n",
    "max_synonyms = 2\n",
    "max_edit_distance = 2  # Set the maximum edit distance for the spell-checking\n",
    "spell = SpellChecker(distance=max_edit_distance)\n",
    "vocabulary = set(df['AB'].str.cat(sep=' ').lower().split())\n",
    "\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=use_synonyms, max_synonyms=max_synonyms)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using synonyms but with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without using synonyms \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# using synonyms and misspelling\n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=True)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Transformer for Medical Text QA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch\n",
    "\n",
    "# Load dataset \n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Path to store precomputed embeddings\n",
    "embeddings_path = 'precomputed_embeddings.npy'\n",
    "\n",
    "# Set up NLTK stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess and embed text in batches\n",
    "def preprocess_and_embed_batch(texts, batch_size=32):\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings in batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = preprocess_and_embed(batch_texts)\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    return np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "# Function to preprocess and embed text\n",
    "def preprocess_and_embed(texts):\n",
    "    # Remove stop words and apply stemming\n",
    "    processed_texts = [' '.join([stemmer.stem(token) for token in tokenizer.tokenize(text) if token not in stop_words]) for text in texts]\n",
    "\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(processed_texts, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():  # This block ensures GPU usage\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the embeddings for the [CLS] token\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def compute_and_save_embeddings(df, embeddings_path, incremental=False):\n",
    "    if incremental and os.path.exists(embeddings_path):\n",
    "        # Load existing embeddings\n",
    "        existing_embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "        # Identify new abstracts to process\n",
    "        new_abstracts = df.loc[~df.index.isin(existing_embeddings.index), 'AB']\n",
    "\n",
    "        if not new_abstracts.empty:\n",
    "            new_embeddings = preprocess_and_embed_batch(new_abstracts)\n",
    "            updated_embeddings = np.concatenate([existing_embeddings, new_embeddings], axis=0)\n",
    "        else:\n",
    "            # Nothing new to process\n",
    "            updated_embeddings = existing_embeddings\n",
    "\n",
    "    else:\n",
    "        # Process all abstracts\n",
    "        updated_embeddings = preprocess_and_embed_batch(df['AB'])\n",
    "\n",
    "    # Save updated embeddings\n",
    "    np.save(embeddings_path, updated_embeddings)\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    return np.load(embeddings_path)\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        if any(token in sentence_tokens for token in query_tokens):\n",
    "            return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, embeddings, num_results=5):\n",
    "    # Preprocess the query\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Calculate embeddings for the query\n",
    "    query_embedding = preprocess_and_embed(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = [cosine_similarity(query_embedding, ae.reshape(1, -1))[0][0] for ae in tqdm(embeddings, desc=\"Calculating similarities\")]\n",
    "    \n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:num_results]\n",
    "    \n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Check if precomputed embeddings exist, otherwise compute and save them\n",
    "if not os.path.exists(embeddings_path):\n",
    "    compute_and_save_embeddings(df, embeddings_path)\n",
    "\n",
    "# Load precomputed embeddings\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without misspelling \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
