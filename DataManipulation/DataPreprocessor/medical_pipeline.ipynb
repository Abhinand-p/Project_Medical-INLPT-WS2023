{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO RUN THIS CELL AS THE DATA IS ALREADY COLLECTED AND SAVE UNDER articles.csv\n",
    "\n",
    "# Data Crawler that works exactly like a human and go one by one through the articles and save the abstracts and references in XML format\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://pubmed.ncbi.nlm.nih.gov/?term=intelligence+%5BTitle%2Fabstract%5D&filter=simsearch1.fha&filter=years.2013-2023&sort=date&size=200\"\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "driver.get(url)\n",
    "\n",
    "# Click the first article to start\n",
    "driver.find_element(By.XPATH, \"//a[@data-ga-action=1]\").click()\n",
    "total_articles = driver.find_element(By.XPATH, \"//*[@id='adjacent-navigation']/div[2]/a/span[1]/span[2]\").text\n",
    "\n",
    "# Find the number of total articles\n",
    "total_articles = re.sub(r\"[^3-9]\",'', total_articles)\n",
    "\n",
    "# Specify the chunk size\n",
    "chunk_size = 1000\n",
    "\n",
    "# Create empty lists to store data\n",
    "titles = []\n",
    "authors = []\n",
    "abstracts = []\n",
    "references_list = []\n",
    "not_found_pages = []\n",
    "\n",
    "for page in tqdm(range(int(total_articles))):\n",
    "    # Extract title of the article\n",
    "    try:\n",
    "        title = driver.find_element(By.CLASS_NAME, \"heading-title\")\n",
    "        if title.is_displayed():\n",
    "            title = title.text\n",
    "    except NoSuchElementException:\n",
    "        title = ''\n",
    "        pass\n",
    "\n",
    "    # Extract autors of the article\n",
    "    try:\n",
    "        authors_elements = driver.find_elements(By.CLASS_NAME, \"full-name\")\n",
    "        author_list = []\n",
    "        if len(authors_elements) > 0:\n",
    "            for author in authors_elements:\n",
    "                author_list.append(author.text)\n",
    "    except NoSuchElementException:\n",
    "        author_list.append('')\n",
    "        pass\n",
    "\n",
    "    # Extract abstract of the article\n",
    "    try:\n",
    "        abstract = driver.find_element(By.ID, \"eng-abstract\")\n",
    "        if abstract.is_displayed():\n",
    "            abstract = abstract.text\n",
    "    except NoSuchElementException:\n",
    "        abstract = ''\n",
    "        pass\n",
    "\n",
    "    # Check and extract if there is reference or are more references \n",
    "    try:\n",
    "        reference = driver.find_element(By.ID, \"references\")\n",
    "        show_all_element = driver.find_element(By.CLASS_NAME, \"show-all\")\n",
    "        if show_all_element.is_displayed():\n",
    "            show_all_element.click()\n",
    "        if reference.is_displayed():\n",
    "            references = driver.find_element(By.CLASS_NAME, \"references-list\").text\n",
    "    except NoSuchElementException:\n",
    "        references = ''\n",
    "        pass\n",
    "\n",
    "    # Append data to lists\n",
    "    titles.append(title)\n",
    "    authors.append(author_list)\n",
    "    abstracts.append(abstract)\n",
    "    references_list.append(references)\n",
    "\n",
    "    if (page + 1) % chunk_size == 0 or page + 1 == int(total_articles):\n",
    "        # Create a DataFrame\n",
    "        data = {\n",
    "                    'Title': pd.Series(titles),\n",
    "                    'Authors': pd.Series(authors),\n",
    "                    'Abstracts': pd.Series(abstracts),\n",
    "                    'References': pd.Series(references_list)\n",
    "                }\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        chunk_number = (page + 1) // chunk_size\n",
    "        csv_filename = f'pubmed_data_chunk_{chunk_number}.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        # Clear lists for the next chunk\n",
    "        titles = []\n",
    "        authors = []\n",
    "        abstracts = []\n",
    "        references_list = []\n",
    "    \n",
    "    # Navigate to the next article\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, \"//div[@class='next side-link visible']\")\n",
    "        if next_page.is_displayed():\n",
    "            next_page.click()\n",
    "    except NoSuchElementException:\n",
    "        not_found_pages.append(page)\n",
    "        print(f\"{page = } not found!\")\n",
    "        pass\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing to have a CSV file with the following columns and respective data in each row:\n",
    "# which will be filtered later to have just useful columns and non dupplicated PMID\n",
    "\n",
    "\"\"\"\n",
    "df shape before cleaning:(74243, 77)\n",
    "df shape after cleaning:(57560, 15)\n",
    "\n",
    "Index(['PMID', 'STAT', 'DRDT', 'CTDT', 'PB', 'DP', 'TI', 'BTI', 'AB', 'CI',\n",
    "       'FED', 'ED', 'FAU', 'AU', 'AD', 'LA', 'PT', 'PL', 'OTO', 'OT', 'EDAT',\n",
    "       'CRDT', 'AID', 'OWN', 'DCOM', 'LR', 'IS', 'VI', 'IP', 'PG', 'LID',\n",
    "       'DEP', 'TA', 'JT', 'JID', 'SB', 'MH', 'MHDA', 'PHST', 'PST', 'SO', 'GR',\n",
    "       'PMC', 'MID', 'COIS', 'TT', 'RN', 'OID', 'SI', 'ISBN', 'CTI', 'CN',\n",
    "       'FIR', 'IR', 'AUID', 'EIN', 'CIN', 'PS', 'FPS', 'CON', 'UOF', 'UIN',\n",
    "       'RIN', 'IRAD', 'EFR', 'OAB', 'OABL', 'PMCR', 'CP', 'ECI', 'DRIN', 'RF',\n",
    "       'EN', 'ROF', 'RPI', 'RPF', 'DDIN'],\n",
    "      dtype='object')\n",
    "\n",
    "\n",
    "Guide to the abbreviations:\n",
    "\n",
    "    PMID: PubMed IDentifier - Unique identifier for a PubMed record.\n",
    "\n",
    "    TI: Title - The title of the article.\n",
    "\n",
    "    AB: Abstract - A brief summary of the article's content.\n",
    "\n",
    "    PB: Publisher - The organization responsible for publishing the article.\n",
    "\n",
    "    FAU: Full Author(s) - Full names of the authors.\n",
    "\n",
    "    FED: Full Editor(s) - Full names of the editors.\n",
    "\n",
    "    DP: Date of Publication - Date when the article was published.\n",
    "\n",
    "    OTO: Other Term Owner - Owner of other terms.\n",
    "\n",
    "    OT: Other Term - Additional terms or keywords associated with the article.\n",
    "\n",
    "    OWN: Owner - Owner of the article.\n",
    "\n",
    "    DCOM: Date Completed - Date when the article was completed.\n",
    "\n",
    "    LR: Last Revision - Last revision date.\n",
    "\n",
    "    JT: Journal Title - Full title of the journal.\n",
    "\n",
    "    MH: MeSH Terms - Medical Subject Headings.\n",
    "\n",
    "    ISBN: International Standard Book Number - ISBN of the article.\n",
    "\n",
    "[Removed]    STAT: Status - Indicates the status of the publication.\n",
    "\n",
    "[Removed]    DRDT: Date Received by Database Transfer - Date when the record was received by the database.\n",
    "\n",
    "[Removed]    CTDT: Current Temporary Date - Current temporary date of the record.\n",
    "\n",
    "[Removed]    BTI: Book Title Indicator - Indicates that the article is part of a book.\n",
    "\n",
    "[Removed]    CI: Copyright Information - Information about the copyright holder.\n",
    "\n",
    "[Removed]    ED: Editor - Abbreviation for the editor.\n",
    "\n",
    "[Removed]    AU: Author - Abbreviation for the author.\n",
    "\n",
    "[Removed]    AD: Author's Affiliation - Affiliation or institution of the author.\n",
    "\n",
    "[Removed]    LA: Language - Language of the article.\n",
    "\n",
    "[Removed]    PT: Publication Type - Type of publication (e.g., Review, Book Chapter).\n",
    "\n",
    "[Removed]    PL: Place of Publication - Location where the article was published.\n",
    "\n",
    "[Removed]    EDAT: Entrez Date - Date the record was added to the Entrez database.\n",
    "\n",
    "[Removed]    CRDT: Create Date - Date the record was created.\n",
    "\n",
    "[Removed]    AID: Article Identifier - Identifier associated with the article.\n",
    "\n",
    "[Removed]    IS: Issue - Issue number of the journal.\n",
    "\n",
    "[Removed]    VI: Volume - Volume number of the journal.\n",
    "\n",
    "[Removed]    IP: Issue Part - Part number of the issue.\n",
    "\n",
    "[Removed]    PG: Page - Page number.\n",
    "\n",
    "[Removed]    LID: Location IDentifier - Identifier for the location of the article.\n",
    "\n",
    "[Removed]    DEP: Date of Electronic Publication - Date of electronic publication.\n",
    "\n",
    "[Removed]    TA: Journal Title (ISO abbreviation) - Title abbreviation of the journal.\n",
    "\n",
    "[Removed]    JID: Journal ID - Identifier for the journal.\n",
    "\n",
    "[Removed]    SB: Subset - Subset designation.\n",
    "\n",
    "[Removed]    MHDA: MeSH Date - MeSH date.\n",
    "\n",
    "[Removed]    PHST: Publication History Status - Publication history status.\n",
    "\n",
    "[Removed]    PST: Publication Status - Publication status.\n",
    "\n",
    "[Removed]    SO: Source - Source of the article.\n",
    "\n",
    "[Removed]    GR: Grant - Grant information.\n",
    "\n",
    "[Removed]    PMC: PubMed Central ID - Identifier for PubMed Central.\n",
    "\n",
    "[Removed]    MID: Manuscript ID - Identifier for the manuscript.\n",
    "\n",
    "[Removed]    COIS: Conflict of Interest Statement - Statement about potential conflicts of interest.\n",
    "\n",
    "[Removed]    TT: Type of Test - Type of test.\n",
    "\n",
    "[Removed]    RN: Registry Number - Registry number.\n",
    "\n",
    "[Removed]    OID: Organization ID - Identifier for the organization.\n",
    "\n",
    "[Removed]    SI: Secondary Source ID - Secondary source identifier.\n",
    "\n",
    "[Removed]    CTI: Current Technology Information - Current technology information.\n",
    "\n",
    "[Removed]    CN: Contract Number - Contract number.\n",
    "\n",
    "[Removed]    FIR: Full Investigator(s) - Full names of the investigators.\n",
    "\n",
    "[Removed]    IR: Investigator - Abbreviation for the investigator.\n",
    "\n",
    "[Removed]    AUID: Author ID - Identifier for the author.\n",
    "\n",
    "[Removed]    EIN: Editor's ID - Identifier for the editor.\n",
    "\n",
    "[Removed]    CIN: Contributor ID - Identifier for the contributor.\n",
    "\n",
    "[Removed]    PS: Personal Name as Subject - Personal name as subject.\n",
    "\n",
    "[Removed]    FPS: Full Personal Name as Subject - Full personal name as subject.\n",
    "\n",
    "[Removed]    CON: Consortium - Consortium information.\n",
    "\n",
    "[Removed]    UOF: Use of Funds - Use of funds information.\n",
    "\n",
    "[Removed]    UIN: Unique Identifier - Unique identifier.\n",
    "\n",
    "[Removed]    RIN: Reviewer ID - Reviewer identifier.\n",
    "\n",
    "[Removed]    IRAD: Investigator Affiliation Department - Investigator affiliation department.\n",
    "\n",
    "[Removed]    EFR: EFS (Endoscopic Frequency Standardization) Factor - EFS factor.\n",
    "\n",
    "[Removed]    OAB: Overall Bank - Overall bank.\n",
    "\n",
    "[Removed]    OABL: Overall Blood - Overall blood.\n",
    "\n",
    "[Removed]    PMCR: PubMed Central Release - PubMed Central release information.\n",
    "\n",
    "[Removed]    CP: Clinical Progress - Clinical progress.\n",
    "\n",
    "[Removed]    ECI: Early Career Investigator - Early career investigator.\n",
    "\n",
    "[Removed]    DRIN: Dual Purpose Experimental Purpose Indicator - Dual-purpose experimental purpose indicator.\n",
    "\n",
    "[Removed]    RF: Release Factor - Release factor.\n",
    "\n",
    "[Removed]    EN: Endorsement - Endorsement.\n",
    "\n",
    "[Removed]    ROF: Reviewer's Office - Reviewer's office.\n",
    "\n",
    "[Removed]    RPI: Reviewer's Position Identifier - Reviewer's position identifier.\n",
    "\n",
    "[Removed]    RPF: Research Performance Factor - Research performance factor.\n",
    "\n",
    "[Removed]    DDIN: Degree-Degree Integration Network - Degree-degree integration network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open('../crawler/articles.txt', 'r', encoding='utf-8') as f:\n",
    "    input_text = f.read()\n",
    "\n",
    "# Split articles based on double quotes\n",
    "articles = re.split(r'\\n\"\\n', input_text.strip())\n",
    "\n",
    "# Define a function to extract data from each article\n",
    "def extract_data(article):\n",
    "    data = {}\n",
    "    current_key = None\n",
    "    current_value = ''\n",
    "\n",
    "    for line in article.split('\\n'):\n",
    "        # matching the key-value pair\n",
    "        match = re.match(r'^([A-Z]{2,4})\\s*- (.+)$', line)\n",
    "\n",
    "        if match:\n",
    "            key, value = match.groups()\n",
    "            if current_key:\n",
    "                # If a key is already set, save the current value\n",
    "                if current_key in data:\n",
    "                    data[current_key] += '|' + current_value\n",
    "                else:\n",
    "                    data[current_key] =  current_value.strip()\n",
    "                current_value = ''  # Reset current value\n",
    "\n",
    "            current_key = key\n",
    "            current_value = value\n",
    "        else:\n",
    "            # If there's no match, append the line to the current value\n",
    "            current_value += '' + line.strip()\n",
    "\n",
    "    # Save the last key-value pair\n",
    "    if current_key:\n",
    "        data[current_key] = current_value.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Extract data from each article\n",
    "article_data_list = [extract_data(article) for article in articles]\n",
    "\n",
    "# Filter out articles without 'AB' key\n",
    "filtered_data_list = [data for data in article_data_list if 'AB' in data]\n",
    "\n",
    "# Create a DataFrame from the filtered data\n",
    "df = pd.DataFrame(filtered_data_list)\n",
    "\n",
    "# Keep only useful columns:  df shape before cleaning:(74243, 77)\n",
    "df = df[['PMID', 'TI', 'AB', 'PB', 'FAU', 'FED', 'DP', 'OTO', 'OT', 'OWN', 'DCOM', 'LR', 'JT', 'MH', 'ISBN']]\n",
    "\n",
    "# Drop duplicates based on the 'PMID' column : df shape after cleaning:(57560, 15)\n",
    "df = df.drop_duplicates(subset='PMID', keep='first')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../crawler/articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic TF-IDF approach and the pandas library for data manipulation:\n",
    "# which will search for the top 5 most similar articles to a given query.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv('articles.csv')\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['AB'].fillna(''))\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, tfidf_matrix, data):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "    document_scores = list(enumerate(cosine_similarities))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        print(f\"Title: {data['TI'].iloc[idx]}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "search(\"IQ scores\", tfidf_matrix, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell Checker and Word Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling\n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Use English stopwords from NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "# Modify the preprocessing step to remove stopwords\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "df['AB'] = df['AB'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# SpellChecker initialization\n",
    "spell = SpellChecker()\n",
    "\n",
    "def correct_spelling(query):\n",
    "    # Tokenize the query\n",
    "    tokens = query.split()\n",
    "\n",
    "    # Correct misspelled words using pyspellchecker\n",
    "    corrected_tokens = [spell.correction(token) for token in tokens]\n",
    "\n",
    "    # Join the corrected tokens back into a corrected query\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    # Tokenize the query\n",
    "    query_tokens = query.lower().split()\n",
    "\n",
    "    # Use regex to split the abstract into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "\n",
    "    # Find the first sentence containing any of the query keywords\n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5):\n",
    "    # Correct misspellings in the query\n",
    "    corrected_query = correct_spelling(query)\n",
    "\n",
    "    # Vectorize the corrected query using the same TF-IDF vectorizer\n",
    "    query_vector = vectorizer.transform([corrected_query])\n",
    "\n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "\n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "\n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF searches for exact matches, and thus suffers from misspellings and synonyms.\n",
    "The purpose of this code cell is to solve those problems by fixing misspellings and replacing all synonyms by one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling using TF-IDF vectorizer for character-level embeddings\n",
    "# as well as enriching the search by adding synonyms for the nouns \n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import spacy\n",
    "\n",
    "# # Load spaCy English language model\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Download NLTK resources\n",
    "# import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# SpellChecker initialization\n",
    "spell = SpellChecker()\n",
    "\n",
    "# NLTK WordNet synonym extraction\n",
    "def get_synonyms(word, pos=None):\n",
    "    # # Map spaCy POS tags to WordNet POS tags\n",
    "    # pos_mapping = {'NOUN': 'n', 'PROPN': 'n', 'VERB': 'v'}\n",
    "    # Map POS tags to WordNet POS tags\n",
    "    pos_mapping = {'NN': 'n', 'VB': 'v'}\n",
    "    pos_tag = pos_mapping.get(pos, 'n') if pos else None\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos_tag):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to replace words in a sentence with their synonyms\n",
    "def replace_with_synonyms(sentence, pos_tags, max_synonyms=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        # Check if the word has a corresponding POS tag\n",
    "        pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "        # If the word has a specific POS tag (e.g., noun), get synonyms\n",
    "        if pos_tag_word and pos_tag_word.startswith(('NN')):\n",
    "            corrected_word = spell.correction(word)\n",
    "            synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])  # Use the first two characters of the tag\n",
    "            print(synonyms)\n",
    "            if synonyms:\n",
    "                # Replace the word with up to max_synonyms synonyms\n",
    "                tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# # Function to replace words in a sentence with their synonyms\n",
    "# def replace_with_synonyms(sentence, max_synonyms=1):\n",
    "#     tokens = word_tokenize(sentence)\n",
    "    \n",
    "#     # Use spaCy for part-of-speech tagging\n",
    "#     pos_tags = [(token.text, token.pos_) for token in nlp(sentence)]\n",
    "    \n",
    "#     for i in range(len(tokens)):\n",
    "#         word = tokens[i]\n",
    "#         # Check if the word has a corresponding POS tag\n",
    "#         pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "#         # If the word has a specific POS tag (e.g., noun), get synonyms\n",
    "#         if pos_tag_word in ['NOUN']:\n",
    "#             corrected_word = spell.correction(word)\n",
    "#             synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])\n",
    "#             print(synonyms)\n",
    "#             if synonyms:\n",
    "#                 # Replace the word with up to max_synonyms synonyms\n",
    "#                 tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "def correct_spelling(query):\n",
    "    # Tokenize the query\n",
    "    tokens = query.split()\n",
    "\n",
    "    # Correct misspelled words using pyspellchecker\n",
    "    corrected_tokens = [spell.correction(token) for token in tokens]\n",
    "\n",
    "    # Join the corrected tokens back into a corrected query\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_query\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    # Tokenize the query\n",
    "    query_tokens = query.lower().split()\n",
    "\n",
    "    # Use regex to split the abstract into sentences\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "\n",
    "    # Find the first sentence containing any of the query keywords\n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "\n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5, use_synonyms=False, max_synonyms=1):\n",
    "    # Correct misspellings in the query\n",
    "    corrected_query = correct_spelling(query)\n",
    "\n",
    "    # Optionally replace words with synonyms\n",
    "    if use_synonyms:\n",
    "        # Perform part-of-speech tagging\n",
    "        pos_tags = pos_tag(word_tokenize(corrected_query))\n",
    "        enriched_query = replace_with_synonyms(corrected_query, pos_tags, max_synonyms=max_synonyms)\n",
    "    else:\n",
    "        enriched_query = corrected_query\n",
    "\n",
    "    # Vectorize the enriched query using the same TF-IDF vectorizer\n",
    "    query_vector = vectorizer.transform([enriched_query])\n",
    "\n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "\n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "\n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "\n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "use_synonyms = True  # Set this flag to control whether to use synonyms or not\n",
    "max_synonyms = 2  # Set the maximum number of synonyms to use for each word\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=use_synonyms, max_synonyms=max_synonyms)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using synonyms but with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without using synonyms \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# using synonyms and misspelling\n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=True)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic method: Levenshtein distance algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search engine with capability of the spell check and correcting the misspelling using TF-IDF vectorizer for character-level embeddings\n",
    "# and edit distance metric-based approach using the Levenshtein distance algorithm\n",
    "# as well as enriching the search by adding synonyms for the nouns \n",
    "\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import Levenshtein as lev\n",
    "\n",
    "# Load your dataset (replace 'your_dataset.csv' with the actual file path)\n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Create a TF-IDF vectorizer for character-level embeddings\n",
    "vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3))\n",
    "tfidf_matrix = vectorizer.fit_transform(df['AB'])\n",
    "\n",
    "# NLTK WordNet synonym extraction\n",
    "def get_synonyms(word, pos=None):\n",
    "    pos_mapping = {'NN': 'n', 'VB': 'v'}\n",
    "    pos_tag = pos_mapping.get(pos, 'n') if pos else None\n",
    "    \n",
    "    synonyms = set()\n",
    "    for syn in wordnet.synsets(word, pos=pos_tag):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    return list(synonyms)\n",
    "\n",
    "# Function to replace words in a sentence with their synonyms\n",
    "def replace_with_synonyms(sentence, pos_tags, max_synonyms=1):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        pos_tag_word = pos_tags[i][1] if i < len(pos_tags) else None\n",
    "        if pos_tag_word and pos_tag_word.startswith(('NN')):\n",
    "            corrected_word = spell.correction(word)\n",
    "            synonyms = get_synonyms(corrected_word, pos=pos_tag_word[:2])\n",
    "            if synonyms:\n",
    "                tokens[i] = ' '.join(synonyms[:max_synonyms])\n",
    "    print(' '.join(tokens))\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def correct_spelling_edit_distance(query):\n",
    "    tokens = query.split()\n",
    "    corrected_tokens = [correct_with_edit_distance(token) for token in tokens]\n",
    "    corrected_query = ' '.join(corrected_tokens)\n",
    "    return corrected_query\n",
    "\n",
    "def correct_with_edit_distance(token):\n",
    "    # Get candidate corrections within a maximum edit distance\n",
    "    candidates = [word for word in vocabulary if lev.distance(token, word) <= max_edit_distance]\n",
    "    \n",
    "    # Choose the candidate with the minimum edit distance\n",
    "    corrected_token = min(candidates, key=lambda x: lev.distance(token, x))\n",
    "    \n",
    "    return corrected_token\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    query_tokens = query.lower().split()\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if any(token in sentence.lower() for token in query_tokens):\n",
    "            return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, tfidf_matrix, num_results=5, use_synonyms=False, max_synonyms=1):\n",
    "    corrected_query = correct_spelling_edit_distance(query)\n",
    "    \n",
    "    if use_synonyms:\n",
    "        pos_tags = pos_tag(word_tokenize(corrected_query))\n",
    "        enriched_query = replace_with_synonyms(corrected_query, pos_tags, max_synonyms=max_synonyms)\n",
    "    else:\n",
    "        enriched_query = corrected_query\n",
    "    \n",
    "    query_vector = vectorizer.transform([enriched_query])\n",
    "    similarities = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
    "    top_indices = similarities.argsort()[-num_results:][::-1]\n",
    "    \n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "use_synonyms = True\n",
    "max_synonyms = 2\n",
    "max_edit_distance = 2  # Set the maximum edit distance for the spell-checking\n",
    "spell = SpellChecker(distance=max_edit_distance)\n",
    "vocabulary = set(df['AB'].str.cat(sep=' ').lower().split())\n",
    "\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=use_synonyms, max_synonyms=max_synonyms)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using synonyms but with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without using synonyms \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# using synonyms and misspelling\n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, tfidf_matrix, use_synonyms=True)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform the abstracts into word embeddings using pre-trained word embeddings (e.g., Word2Vec, GloVe, or FastText) and calculate the cosine similarity between the query and the abstracts based on the word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Transformer for Medical Text QA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch\n",
    "\n",
    "# Load dataset \n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Path to store precomputed embeddings\n",
    "embeddings_path = 'precomputed_embeddings.npy'\n",
    "\n",
    "# Set up NLTK stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess and embed text in batches\n",
    "def preprocess_and_embed_batch(texts, batch_size=32):\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings in batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = preprocess_and_embed(batch_texts)\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    return np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "# Function to preprocess and embed text\n",
    "def preprocess_and_embed(texts):\n",
    "    # Remove stop words and apply stemming\n",
    "    processed_texts = [' '.join([stemmer.stem(token) for token in tokenizer.tokenize(text) if token not in stop_words]) for text in texts]\n",
    "\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(processed_texts, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():  # This block ensures GPU usage\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the embeddings for the [CLS] token\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def compute_and_save_embeddings(df, embeddings_path, incremental=False):\n",
    "    if incremental and os.path.exists(embeddings_path):\n",
    "        # Load existing embeddings\n",
    "        existing_embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "        # Identify new abstracts to process\n",
    "        new_abstracts = df.loc[~df.index.isin(existing_embeddings.index), 'AB']\n",
    "\n",
    "        if not new_abstracts.empty:\n",
    "            new_embeddings = preprocess_and_embed_batch(new_abstracts)\n",
    "            updated_embeddings = np.concatenate([existing_embeddings, new_embeddings], axis=0)\n",
    "        else:\n",
    "            # Nothing new to process\n",
    "            updated_embeddings = existing_embeddings\n",
    "\n",
    "    else:\n",
    "        # Process all abstracts\n",
    "        updated_embeddings = preprocess_and_embed_batch(df['AB'])\n",
    "\n",
    "    # Save updated embeddings\n",
    "    np.save(embeddings_path, updated_embeddings)\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    return np.load(embeddings_path)\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        if any(token in sentence_tokens for token in query_tokens):\n",
    "            return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, embeddings, num_results=5):\n",
    "    # Preprocess the query\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Calculate embeddings for the query\n",
    "    query_embedding = preprocess_and_embed(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = [cosine_similarity(query_embedding, ae.reshape(1, -1))[0][0] for ae in tqdm(embeddings, desc=\"Calculating similarities\")]\n",
    "    \n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:num_results]\n",
    "    \n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Check if precomputed embeddings exist, otherwise compute and save them\n",
    "if not os.path.exists(embeddings_path):\n",
    "    compute_and_save_embeddings(df, embeddings_path)\n",
    "\n",
    "# Load precomputed embeddings\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without misspelling \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conbine different columns of the dataset into one column\n",
    "import pandas as pd\n",
    "\n",
    "# Read the original CSV file\n",
    "df_part = pd.read_csv('articles.csv', index_col='PMID', usecols=['PMID', 'TI', 'AB', 'FAU', 'DP', 'OT', 'JT', 'MH'])\n",
    "\n",
    "# Create a new DataFrame with the desired structure\n",
    "new_df = pd.DataFrame(index=df_part.index)\n",
    "\n",
    "# Combine the information into a single column\n",
    "new_df['Combined_Info'] = (\n",
    "    'Title: ' + df_part['TI'].fillna('None') + '\\n' +\n",
    "    'Abstract: ' + df_part['AB'].fillna('None') + '\\n' +\n",
    "    'Authors: ' + df_part['FAU'].fillna('None') + '\\n' +\n",
    "    'Data of Publication: ' + df_part['DP'].fillna('None') + '\\n' +\n",
    "    'Terms or keywords associated with the article: ' + df_part['OT'].fillna('None') + '\\n' +\n",
    "    'Journal Title: ' + df_part['JT'].fillna('None') + '\\n' +\n",
    "    'Medical subject headings: ' + df_part['MH'].fillna('None')\n",
    ")\n",
    "\n",
    "# Save the new DataFrame to a CSV file\n",
    "new_df.to_csv('combined_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the large CSV file into smaller chunks\n",
    "import pandas as pd\n",
    "\n",
    "def split_csv(input_csv, output_prefix, chunk_size):\n",
    "    # Read the large CSV file into a pandas DataFrame\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Determine the number of chunks needed\n",
    "    num_chunks = (len(df) // chunk_size) + 1\n",
    "\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df[i * chunk_size:(i + 1) * chunk_size] for i in range(num_chunks)]\n",
    "\n",
    "    # Save each chunk as a separate CSV file\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        output_csv = f\"{output_prefix}_{i + 1}.csv\"\n",
    "        chunk.to_csv(output_csv, index=False)\n",
    "        print(f\"Chunk {i + 1} saved to {output_csv}\")\n",
    "\n",
    "# Example usage\n",
    "input_csv_path = 'data_1.csv'  # Replace with the path to your large CSV file\n",
    "output_prefix = 'sub_data'  # Prefix for the output CSV files\n",
    "chunk_size = 1000  # Number of rows per chunk\n",
    "\n",
    "split_csv(input_csv_path, output_prefix, chunk_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding the abstracts using the pre-trained BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the abstracts using BERT and saving them to a file\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the abstracts\n",
    "def encode_abstracts_sliding_window(abstracts, window_size=512, stride=256):\n",
    "    encoded_abstracts = []\n",
    "\n",
    "    for abstract in tqdm(abstracts, desc=\"Encoding Abstracts\", unit=\"abstract\"):\n",
    "        tokens = tokenizer.tokenize(abstract)\n",
    "        total_length = len(tokens)\n",
    "\n",
    "        # Determine the number of overlapping windows\n",
    "        num_windows = abs(total_length - window_size) // stride + 1\n",
    "\n",
    "        for i in range(0, num_windows * stride, stride):\n",
    "            # Extract a window of tokens\n",
    "            window_tokens = tokens[i:i + window_size]\n",
    "\n",
    "            # Convert tokens back to a string\n",
    "            window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "\n",
    "            # Tokenize and encode the window\n",
    "            inputs = tokenizer(window_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            encoded_abstracts.append(outputs.last_hidden_state.mean(dim=1))\n",
    "\n",
    "    if not encoded_abstracts:\n",
    "        print(\"No encoded abstracts found.\")\n",
    "    return torch.cat(encoded_abstracts, dim=0)\n",
    "\n",
    "\n",
    "# Function to save encoded abstracts\n",
    "def save_encoded_abstracts(encoded_abstracts, filename):\n",
    "    torch.save(encoded_abstracts, filename)\n",
    "\n",
    "# Function to load encoded abstracts\n",
    "def load_encoded_abstracts(filename):\n",
    "    return torch.load(filename)\n",
    "\n",
    "# Example: Load, encode, and save each part separately\n",
    "for i in tqdm(range(1, 11), desc=\"Processing Parts\", unit=\"part\"):\n",
    "    file_path = f'sub_data_{i}.csv'\n",
    "    df_part = pd.read_csv(file_path)\n",
    "\n",
    "    # Encode abstracts\n",
    "    encoded_abstracts_part = encode_abstracts_sliding_window(df_part['Combined_Info'])\n",
    "\n",
    "    # Save encoded abstracts\n",
    "    save_encoded_abstracts(encoded_abstracts_part, f'encoded_data_part_{i}.pt')\n",
    "\n",
    "# Load and concatenate encoded abstracts from all parts\n",
    "encoded_abstracts_parts = []\n",
    "for i in tqdm(range(1, 11), desc=\"Loading Parts\", unit=\"part\"):\n",
    "    encoded_abstracts_part = load_encoded_abstracts(f'encoded_data_part_{i}.pt')\n",
    "    encoded_abstracts_parts.append(encoded_abstracts_part)\n",
    "\n",
    "# Concatenate the parts\n",
    "encoded_abstracts = torch.cat(encoded_abstracts_parts, dim=0)\n",
    "\n",
    "# Save the encoded_abstracts tensor\n",
    "torch.save(encoded_abstracts, 'encoded_data.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering System using BERT pretrained on SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"retrieving the most similar abstracts to a question and then generating an answer based on those abstracts is a \n",
    "reasonable strategy for a Question Answering (QA) system. While the SQuAD (Stanford Question Answering Dataset) \n",
    "is typically used for training and evaluating QA models, you can adapt your approach to leverage \n",
    "the idea of retrieving relevant passages or abstracts and then generating answers.\n",
    "\n",
    "Steps:\n",
    "\n",
    "    1-Retrieve Similar Abstracts:\n",
    "        Use a method (such as cosine similarity) to retrieve the top N most similar abstracts to a given question from your collection of abstracts.\n",
    "\n",
    "    2-Generate Answers:\n",
    "        For each of the retrieved abstracts, use a QA model to generate answers to the question.\n",
    "        Fine-tune a pre-trained QA model on your specific dataset, considering the structure of your abstracts and questions.\n",
    "\n",
    "    3-Combine Answers:\n",
    "        Aggregate or combine the answers generated from different abstracts to provide a final answer.\"\"\"\n",
    "\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "\n",
    "# Tokenize and encode the abstracts\n",
    "def encode_abstracts_sliding_window(abstracts, window_size=512, stride=256):\n",
    "    encoded_abstracts = []\n",
    "\n",
    "    for abstract in tqdm(abstracts, desc=\"Encoding Abstracts\", unit=\"abstract\"):\n",
    "        tokens = tokenizer.tokenize(abstract)\n",
    "        total_length = len(tokens)\n",
    "\n",
    "        # Determine the number of overlapping windows\n",
    "        num_windows = abs(total_length - window_size) // stride + 1\n",
    "\n",
    "        for i in range(0, num_windows * stride, stride):\n",
    "            # Extract a window of tokens\n",
    "            window_tokens = tokens[i:i + window_size]\n",
    "\n",
    "            # Convert tokens back to a string\n",
    "            window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "\n",
    "            # Tokenize and encode the window\n",
    "            inputs = tokenizer(window_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            encoded_abstracts.append(outputs.last_hidden_state.mean(dim=1))\n",
    "\n",
    "    if not encoded_abstracts:\n",
    "        print(\"No encoded abstracts found.\")\n",
    "    return torch.cat(encoded_abstracts, dim=0)\n",
    "\n",
    "\n",
    "# Function to load encoded abstracts\n",
    "def load_encoded_abstracts(filename):\n",
    "    return torch.load(filename)\n",
    "\n",
    "# Function to retrieve top k similar abstracts\n",
    "def retrieve_top_k_abstracts(query, abstracts, df, k=5):\n",
    "    # Encode the query using the sliding window approach (as before)\n",
    "    query_embedding = encode_abstracts_sliding_window([query])\n",
    "    \n",
    "    # Calculate cosine similarity between the query and encoded abstracts\n",
    "    similarities = cosine_similarity(query_embedding, abstracts)\n",
    "    \n",
    "    # Get the indices of the top k most similar abstracts\n",
    "    top_k_indices = similarities.argsort()[0, -k:][::-1]\n",
    "\n",
    "    if len(top_k_indices) == 0:\n",
    "        print(\"No matching abstracts found.\")\n",
    "        return []\n",
    "\n",
    "    # Print some information for debugging\n",
    "    print(\"Top k PMIDs:\", df.index[top_k_indices].tolist())\n",
    "    print(\"Abstract lengths:\", [len(df.loc[pmid, 'Combined_Info']) for pmid in df.index[top_k_indices]])\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "\n",
    "# Function to generate answers using the QA model\n",
    "def generate_answers(question, abstracts, df):\n",
    "    answers = []\n",
    "\n",
    "    for index in abstracts:\n",
    "        # Get the PMID\n",
    "        pmid = df.index[index]\n",
    "\n",
    "        # Get the abstract text\n",
    "        abstract_text = df.loc[pmid, 'Combined_Info']\n",
    "\n",
    "        # Tokenize and encode the question and abstract\n",
    "        inputs = qa_tokenizer(question, abstract_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "        # Perform inference with the QA model\n",
    "        with torch.no_grad():\n",
    "            outputs = qa_model(**inputs)\n",
    "\n",
    "        # Get the predicted answer\n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "        answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Using PMID as the index column\n",
    "df_part = pd.read_csv('data_1.csv', index_col='PMID')\n",
    "# df_part = pd.read_csv('articles.csv', index_col='PMID', usecols=['TI', 'AB', 'FAU', 'DP', 'OT', 'JT', 'MH'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "encoded_abstracts = load_encoded_abstracts('encoded_data.pt')\n",
    "question = \"what is Artificial Intelligence?\"\n",
    "top_k_abstracts = retrieve_top_k_abstracts(question, encoded_abstracts, df_part, k=5)\n",
    "\n",
    "# Print the top 5 similar abstracts\n",
    "print(\"Top 5 Similar Abstracts:\")\n",
    "for index in top_k_abstracts:\n",
    "    pmid = df_part.index[index]\n",
    "    print(\"PMID:\", pmid)\n",
    "    print(\"Abstract:\", df_part.loc[pmid, 'Combined_Info'])\n",
    "\n",
    "answers = generate_answers(question, top_k_abstracts, df_part)\n",
    "\n",
    "# Display the generated answers\n",
    "print(\"\\nGenerated Answers:\")\n",
    "for answer in answers:\n",
    "    print(answer)\n",
    "#[TODO] check if the retrieved data is correct as sometimes it produces some IDs that are not part of the dataset, e.g. query = \"who is Chenq?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Conbine different columns of the dataset into one column\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the original CSV file\n",
    "# df_part = pd.read_csv('articles.csv', index_col='PMID', usecols=['PMID', 'TI', 'AB', 'FAU', 'DP', 'OT', 'JT', 'MH'])\n",
    "\n",
    "# # Create a new DataFrame with the desired structure\n",
    "# new_df = pd.DataFrame(index=df_part.index)\n",
    "\n",
    "# # Combine the information into a single column\n",
    "# new_df['CD'] = (\n",
    "#     'PMID: ' + df_part.index.astype(str) + ', ' +\n",
    "#     'Abstract: ' + df_part['AB'].fillna('None') + ', ' +\n",
    "#     'Title: ' + df_part['TI'].fillna('None') + ', ' +\n",
    "#     'Authors: ' + df_part['FAU'].fillna('None') + ', ' +\n",
    "#     'Data of Publication: ' + df_part['DP'].fillna('None') + ', ' +\n",
    "#     'Terms or keywords associated with the article: ' + df_part['OT'].fillna('None') + ', ' +\n",
    "#     'Journal Title: ' + df_part['JT'].fillna('None') + ', ' +\n",
    "#     'Medical subject headings: ' + df_part['MH'].fillna('None') + ', '# +\n",
    "#     # 'Abstract: ' + df_part['AB'].fillna('None')\n",
    "# )\n",
    "# new_df['source'] = 'https://pubmed.ncbi.nlm.nih.gov/' + df_part.index.astype(str)\n",
    "# # Save the new DataFrame to a CSV file\n",
    "# new_df.to_csv('additional_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CD</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24278995</th>\n",
       "      <td>PMID: 24278995, Abstract: CLINICAL CHARACTERIS...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/24278995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25529590</th>\n",
       "      <td>PMID: 25529590, Abstract: This study utilized ...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/25529590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25529585</th>\n",
       "      <td>PMID: 25529585, Abstract: Interpretation of th...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/25529585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25284715</th>\n",
       "      <td>PMID: 25284715, Abstract: Despite the importan...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/25284715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25265311</th>\n",
       "      <td>PMID: 25265311, Abstract: Neuropsychological a...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/25265311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         CD  \\\n",
       "PMID                                                          \n",
       "24278995  PMID: 24278995, Abstract: CLINICAL CHARACTERIS...   \n",
       "25529590  PMID: 25529590, Abstract: This study utilized ...   \n",
       "25529585  PMID: 25529585, Abstract: Interpretation of th...   \n",
       "25284715  PMID: 25284715, Abstract: Despite the importan...   \n",
       "25265311  PMID: 25265311, Abstract: Neuropsychological a...   \n",
       "\n",
       "                                            source  \n",
       "PMID                                                \n",
       "24278995  https://pubmed.ncbi.nlm.nih.gov/24278995  \n",
       "25529590  https://pubmed.ncbi.nlm.nih.gov/25529590  \n",
       "25529585  https://pubmed.ncbi.nlm.nih.gov/25529585  \n",
       "25284715  https://pubmed.ncbi.nlm.nih.gov/25284715  \n",
       "25265311  https://pubmed.ncbi.nlm.nih.gov/25265311  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Conbine different columns of the dataset into one column\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the original CSV file\n",
    "# df_part = pd.read_csv('articles.csv', index_col='PMID', usecols=['PMID', 'TI', 'AB', 'FAU', 'DP', 'OT', 'JT', 'MH'])\n",
    "\n",
    "# # Create a new DataFrame with the desired structure\n",
    "# new_df = pd.DataFrame(index=df_part.index)\n",
    "\n",
    "# # Combine the information into a single column\n",
    "# new_df['CD'] = (\n",
    "#     'PMID: ' + df_part.index.astype(str) + '\\n' +\n",
    "#     'Abstract: ' + df_part['AB'].fillna('None') + '\\n' +\n",
    "#     'Title: ' + df_part['TI'].fillna('None') + '\\n' +\n",
    "#     'Authors: ' + df_part['FAU'].fillna('None') + ',\\n' +\n",
    "#     'Data of Publication: ' + df_part['DP'].fillna('None') + '\\n' +\n",
    "#     'Terms or keywords associated with the article: ' + df_part['OT'].fillna('None') + '\\n' +\n",
    "#     'Journal Title: ' + df_part['JT'].fillna('None') + '\\n' +\n",
    "#     'Medical subject headings: ' + df_part['MH'].fillna('None') + '\\n'# +\n",
    "#     # 'Abstract: ' + df_part['AB'].fillna('None')\n",
    "# )\n",
    "# new_df['source'] = 'https://pubmed.ncbi.nlm.nih.gov/' + df_part.index.astype(str)\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# # # Read your DataFrame from a CSV file\n",
    "# # df = pd.read_csv('your_dataframe.csv')\n",
    "\n",
    "# # Function to filter out lines ending with 'None' from a given text\n",
    "# def filter_lines(text):\n",
    "#     lines = text.split('\\n')\n",
    "#     filtered_lines = [line for line in lines if not line.strip().endswith('None')]\n",
    "#     return ', '.join(filtered_lines)\n",
    "\n",
    "# # Apply the filtering function to each row in the 'CD' column\n",
    "# new_df['CD'] = new_df['CD'].apply(filter_lines)\n",
    "\n",
    "# # Save the new DataFrame to a CSV file\n",
    "# new_df.to_csv('additional_data.csv')\n",
    "# # Print the DataFrame with the filtered 'CD' column\n",
    "# new_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # [TODO]Add source of each article using https://pubmed.ncbi.nlm.nih.gov/PMID\n",
    "\n",
    "# docs['source'] = 'https://pubmed.ncbi.nlm.nih.gov/' + docs['PMID'].astype(str)\n",
    "# # save the data\n",
    "# docs.to_csv('data_0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Behro\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1900: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 75\n",
      "Avg: 560\n",
      "Max: 31071\n"
     ]
    }
   ],
   "source": [
    "# [TODO]Upload data to the Hugging Face Datasets library\n",
    "# [TODO]Load data from hugging face datasets\n",
    "# [TODO]Add source of each article using https://pubmed.ncbi.nlm.nih.gov/PMID\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import LlamaTokenizer\n",
    "docs = pd.read_csv('additional_data.csv')\n",
    "# docs['Combined_Info'] = docs['Combined_Info'].str.replace('|', ' ')\n",
    "\n",
    "hf_auth = os.environ.get('HF_AUTH')\n",
    "#[TODO] tokenize for GPT-3.5 Turbo\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\",use_auth_token=hf_auth)\n",
    "# length of tokenized input\n",
    "def token_len(text):\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "# stats for tokenized input But not necessary\n",
    "token_counts = [token_len(docs['CD'][i]) for i, _ in enumerate(docs['CD'])]\n",
    "min_tokens=min(token_counts)\n",
    "avg_tokens=int(sum(token_counts) / len(token_counts))\n",
    "max_tokens=max(token_counts)\n",
    "print(f\"\"\"Min: {min_tokens}\n",
    "Avg: {avg_tokens}\n",
    "Max: {max_tokens}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.convert_ids_to_tokens(tokenizer(\"PMID 24278995 Title CASK Disorders Authors Moog Ute Kutsche Kerstin Data of Publication 1993 Terms or keywords associated with the article Intellectual Disability and Microcephaly with Pontine and Cerebellar Hypoplasia MICPCH XLinked Intellectual Disability XLID with or without Nystagmus Peripheral plasma membrane\").input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of chunk: 3\n",
      "Content of chunk0:\n",
      "PMID: 24278995, Abstract: CLINICAL CHARACTERISTICS: CASK disorders include a spectrum of phenotypes in both females and males. Two main types of clinical presentation are seen: Microcephalywith pontine and cerebellar hypoplasia (MICPCH), generally associated withpathogenic loss-of-function variants in CASK. X-linked intellectual disability(XLID) with or without nystagmus, generally associated with hypomorphic CASKpathogenic variants. MICPCH is typically seen in females with moderate-to-severeintellectual disability, progressive microcephaly with or without ophthalmologicanomalies, and sensorineural hearing loss. Most are able to sit independently;20%-25% attain the ability to walk; language is nearly absent in most. Neurologicfeatures may include axial hypotonia, hypertonia/spasticity of the extremities,and dystonia or other movement disorders. Nearly 40% have seizures by age tenyears. Behaviors may include sleep disturbances, hand stereotypies, and selfbiting. MICPCH in males may occur with or without severe epileptic encephalopathyin addition to severe-to-profound developmental delay. When seizures are presentthey occur early and may be intractable. In individuals and families with milder(i.e., hypomorphic) pathogenic variants, the clinical phenotype is usually thatof XLID with or without nystagmus and additional clinical features. Males havemild-to-severe intellectual disability, with or without nystagmus and otherocular features. Females typically have normal intelligence with some displayingmild-to-severe intellectual disability with or without ocular features.DIAGNOSIS/TESTING: The diagnosis of a CASK disorder is established in a femalewho is heterozygous for a CASK pathogenic variant and in a male who is hemizygousfor a CASK pathogenic variant on molecular genetic testing. Rarely, affectedmales have a mosaic pathogenic\n",
      "the length of chunk 0 is: 1857\n",
      "****************************************************************************************************\n",
      "Content of chunk1:\n",
      "testing. Rarely, affectedmales have a mosaic pathogenic variant. MANAGEMENT: Treatment of manifestations:Treatment is symptomatic and includes standard management of developmental delayand intellectual disability issues; medication for seizures; nutritional support;use of physiotherapy; and treatment of abnormal vision or hearing loss. GENETICCOUNSELING: CASK disorders are inherited in an X-linked manner. Risk to thefamily members of a proband with a CASK disorder depends on the phenotype (i.e.,MICPCH or XLID +/- nystagmus) in the proband. MICPCH. Most affected females andmales represent simplex cases (i.e., the only affected family member) and havethe disorder as the result of a de novo CASK pathogenic variant. Becauseheterozygous females manifest the phenotype, an asymptomatic mother is unlikelyto be heterozygous for the CASK pathogenic variant. If a proband represents asimplex case, the recurrence risk to sibs appears to be low but greater than thatof the general population because of the possibility of parental germlinemosaicism. XLID +/- nystagmus. The father of a male with a CASK disorder will nothave the disorder nor will he be hemizygous for the CASK pathogenic variant. If amale is the only affected family member, the mother may be a heterozygote or theaffected male may have a de novo pathogenic variant. In a family with more thanone affected individual, the mother of an affected male is an obligateheterozygote. If the mother of the proband has a CASK pathogenic variant, thechance of transmitting it in each pregnancy is 50%: males who inherit thepathogenic variant will be affected; females who inherit the pathogenic variantwill typically be asymptomatic but\n",
      "the length of chunk 1 is: 1693\n",
      "****************************************************************************************************\n",
      "Content of chunk1:\n",
      "who inherit the pathogenic variantwill typically be asymptomatic but may have a range of manifestations. If theCASK pathogenic variant cannot be detected in maternal leukocyte DNA, the risk tosibs is greater than that of the general population because of the possibility ofparental germline mosaicism. Once the CASK pathogenic variant has been identifiedin an affected family member, prenatal testing for a pregnancy at increased riskand preimplantation genetic testing for a CASK disorder are possible., Title: CASK Disorders., Authors: Moog, Ute|Kutsche, Kerstin,, Data of Publication: 1993, Terms or keywords associated with the article: Intellectual Disability and Microcephaly with Pontine and Cerebellar Hypoplasia (MICPCH)|X-Linked Intellectual Disability (XLID) with or without Nystagmus|Peripheral plasma membrane protein CASK|CASK|CASK Disorders,\n",
      "the length of chunk 2 is: 856\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Priority is by the length of chunk and overlap,\n",
    "# if they don't exceed the default values, the separator will be used\n",
    "from langchain.text_splitter import NLTKTextSplitter,CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=40,\n",
    "    length_function=token_len,\n",
    "    # separators=['\\n\\n', '\\n', ' ', '']\n",
    ")\n",
    "\n",
    "# test the text splitter\n",
    "chunks = text_splitter.split_text(docs['CD'][0])\n",
    "print(f\"length of chunk: {len(chunks)}\")\n",
    "print(f\"Content of chunk0:\\n{chunks[0]}\")\n",
    "print('the length of chunk 0 is:', len(chunks[0]))\n",
    "print(\"*\"*100)\n",
    "print(f\"Content of chunk1:\\n{chunks[1]}\")\n",
    "print('the length of chunk 1 is:', len(chunks[1]))\n",
    "print(\"*\"*100)\n",
    "print(f\"Content of chunk1:\\n{chunks[2]}\")\n",
    "print('the length of chunk 2 is:', len(chunks[2]))\n",
    "print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[3]}\")\n",
    "# print('the length of chunk 3 is:', len(chunks[3]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[4]}\")\n",
    "# print('the length of chunk 4 is:', len(chunks[4]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[5]}\")\n",
    "# print('the length of chunk 5 is:', len(chunks[5]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[6]}\")\n",
    "# print('the length of chunk 6 is:', len(chunks[6]))\n",
    "# print(\"*\"*100)\n",
    "# print(f\"Content of chunk1:\\n{chunks[7]}\")\n",
    "# print('the length of last chunk is:', len(chunks[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     # Define the pattern to match punctuation\n",
    "#     punctuation_pattern = r'[^\\w\\s]'\n",
    "    \n",
    "#     # Use regex to substitute punctuation with an empty string\n",
    "#     text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    \n",
    "#     return text_without_punctuation\n",
    "\n",
    "# # Example usage:\n",
    "# text = \"PMID: 24278995, Title: CASK Disorders., Authors: Moog, Ute Kutsche, Kerstin, Data of Publication: 1993, Terms or keywords associated with the article: Intellectual Disability and Microcephaly with Pontine and Cerebellar Hypoplasia (MICPCH) X-Linked Intellectual Disability (XLID) with or without Nystagmus Peripheral plasma membrane\"\n",
    "# cleaned_text = remove_punctuation(text)\n",
    "# print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57560it [1:01:47, 15.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93776"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# import re\n",
    "\n",
    "# def remove_punctuation(text):\n",
    "#     # Define the pattern to match punctuation\n",
    "#     punctuation_pattern = r'[^\\w\\s]'\n",
    "    \n",
    "#     # Use regex to substitute punctuation with an empty string\n",
    "#     text_without_punctuation = re.sub(punctuation_pattern, '', text)\n",
    "    \n",
    "#     return text_without_punctuation\n",
    "\n",
    "documents=[]\n",
    "for j, doc in tqdm(enumerate(docs['CD'])):\n",
    "    # chunks = text_splitter.split_text(remove_punctuation(doc))\n",
    "    chunks = text_splitter.split_text(doc)\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        documents.append({\n",
    "            'id': f\"{docs['PMID'][j]}-{i}\",\n",
    "            'text': chunk,\n",
    "            'source': docs['source'][j],\n",
    "        })\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24278995-0</td>\n",
       "      <td>PMID: 24278995, Abstract: CLINICAL CHARACTERIS...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/24278995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24278995-1</td>\n",
       "      <td>testing. Rarely, affectedmales have a mosaic p...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/24278995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24278995-2</td>\n",
       "      <td>who inherit the pathogenic variantwill typical...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/24278995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25529590-0</td>\n",
       "      <td>PMID: 25529590, Abstract: This study utilized ...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/25529590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25529585-0</td>\n",
       "      <td>PMID: 25529585, Abstract: Interpretation of th...</td>\n",
       "      <td>https://pubmed.ncbi.nlm.nih.gov/25529585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0  24278995-0  PMID: 24278995, Abstract: CLINICAL CHARACTERIS...   \n",
       "1  24278995-1  testing. Rarely, affectedmales have a mosaic p...   \n",
       "2  24278995-2  who inherit the pathogenic variantwill typical...   \n",
       "3  25529590-0  PMID: 25529590, Abstract: This study utilized ...   \n",
       "4  25529585-0  PMID: 25529585, Abstract: Interpretation of th...   \n",
       "\n",
       "                                     source  \n",
       "0  https://pubmed.ncbi.nlm.nih.gov/24278995  \n",
       "1  https://pubmed.ncbi.nlm.nih.gov/24278995  \n",
       "2  https://pubmed.ncbi.nlm.nih.gov/24278995  \n",
       "3  https://pubmed.ncbi.nlm.nih.gov/25529590  \n",
       "4  https://pubmed.ncbi.nlm.nih.gov/25529585  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the list of dictionaries to a DataFrame\n",
    "import pandas as pd\n",
    "data = pd.DataFrame(documents)\n",
    "data.to_csv('data_llama_recursive_1200_40.csv', index=False)\n",
    "data = pd.read_csv('data_llama_recursive_1200_40.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the the top 5 relevent articles for a given query using TF-IDF and cosine similarity\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load your CSV data\n",
    "# additional_docs = pd.read_csv('additional_data.csv')\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "# Using charactor based vectorizer cannot find the names of the authors\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['text'].fillna(''))\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, tfidf_matrix, data):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "    document_scores = list(enumerate(cosine_similarities))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        if score != 0:\n",
    "            print(f\"ID: {data['id'].iloc[idx]}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 28973399-1, Score: 0.15922436691226732\n",
      "ID: 24278995-2, Score: 0.11699660887872798\n",
      "ID: 30642688-0, Score: 0.09687613918535377\n",
      "ID: 34698500-1, Score: 0.08939465596638295\n",
      "ID: 35434225-0, Score: 0.0858795629247108\n"
     ]
    }
   ],
   "source": [
    "# Example query\n",
    "search(\"who is Moog?\", tfidf_matrix, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 24278995-2, Score: 15.563824888287513\n",
      "ID: 28973399-1, Score: 13.237401367851781\n",
      "ID: 30642688-0, Score: 11.251462229953042\n",
      "ID: 35434225-0, Score: 9.726458452054661\n",
      "ID: 34698500-1, Score: 9.553797854351048\n"
     ]
    }
   ],
   "source": [
    "# BM25 implementation, that is not recommendeed \n",
    "# as it produce the same result but takes much longer time \n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import sparse\n",
    "\n",
    "\n",
    "class BM25(object):\n",
    "    def __init__(self, b=0.75, k1=1.6):\n",
    "        self.vectorizer = TfidfVectorizer(norm=None, smooth_idf=False)\n",
    "        self.b = b\n",
    "        self.k1 = k1\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Fit IDF to documents X \"\"\"\n",
    "        self.vectorizer.fit(X)\n",
    "        y = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        self.avdl = y.sum(1).mean()\n",
    "\n",
    "    def transform(self, q, X):\n",
    "        \"\"\" Calculate BM25 between query q and documents X \"\"\"\n",
    "        b, k1, avdl = self.b, self.k1, self.avdl\n",
    "\n",
    "        # apply CountVectorizer\n",
    "        X = super(TfidfVectorizer, self.vectorizer).transform(X)\n",
    "        len_X = X.sum(1).A1\n",
    "        q, = super(TfidfVectorizer, self.vectorizer).transform([q])\n",
    "        assert sparse.isspmatrix_csr(q)\n",
    "\n",
    "        # convert to csc for better column slicing\n",
    "        X = X.tocsc()[:, q.indices]\n",
    "        denom = X + (k1 * (1 - b + b * len_X / avdl))[:, None]\n",
    "        # idf(t) = log [ n / df(t) ] + 1 in sklearn, so it need to be coneverted\n",
    "        # to idf(t) = log [ n / df(t) ] with minus 1\n",
    "        idf = self.vectorizer._tfidf.idf_[None, q.indices] - 1.\n",
    "        numer = X.multiply(np.broadcast_to(idf, X.shape)) * (k1 + 1)                                                          \n",
    "        return (numer / denom).sum(1).A1\n",
    "\n",
    "# Example usage\n",
    "\n",
    "query = \"who is Moog?\"\n",
    "# search(query, bm25_matrix, data)\n",
    "def search(query, data):\n",
    "    bm25 = BM25()\n",
    "    bm25.fit(data['text'])\n",
    "    scores = bm25.transform(query, data['text'])\n",
    "    document_scores = list(enumerate(scores))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        if score != 0:\n",
    "            print(f\"ID: {data['id'].iloc[idx]}, Score: {score}\")\n",
    "\n",
    "search(query, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 32338619-0, Score: 5.653210806314239\n",
      "ID: 32428260-0, Score: 5.6218955080845925\n",
      "ID: 36905588-0, Score: 5.444500880099623\n",
      "ID: 36072043-0, Score: 5.353867915241998\n",
      "ID: 24864183-0, Score: 5.302817414684655\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Load your CSV data\n",
    "# additional_docs = pd.read_csv('additional_data.csv')\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenized_texts = [text.split() for text in data['text'].fillna('')]\n",
    "\n",
    "# Create a BM25 model\n",
    "bm25 = BM25Okapi(tokenized_texts)\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, bm25, tokenized_texts, data):\n",
    "    scores = bm25.get_scores(query.split())\n",
    "    top_results = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)[:5]\n",
    "    for idx, score in top_results:\n",
    "        if score != 0:\n",
    "            print(f\"ID: {data['id'].iloc[idx]}, Score: {score}\")\n",
    "\n",
    "# Example usage\n",
    "query = \"who is Moog?\"\n",
    "search(query, bm25, tokenized_texts, data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from pinecone import Pinecone, PodSpec\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Behro\\anaconda3\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "# we don't use OpenAI embedding as it costs money  multi-qa-mpnet-base-dot-v1\n",
    "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2' #all-mpnet-base-v2'\n",
    "\n",
    "device = 'cuda:0' # make sure you are on gpu\n",
    "batch_size = 32\n",
    "embed_model = HuggingFaceEmbeddings(\n",
    "    model_name=embedding_model,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'device': device, 'batch_size': batch_size}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load pre-trained BERT model and tokenizer\n",
    "# embedding_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "# encoded_abstracts = []\n",
    "# for i, txt in tqdm(enumerate(data['text'])):\n",
    "#     # Tokenize and encode the window\n",
    "#     inputs = tokenizer(txt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = embedding_model(**inputs)\n",
    "#         encoded_abstracts.append(outputs.last_hidden_state.mean(dim=1))\n",
    "# embeddings= torch.cat(encoded_abstracts, dim=0)\n",
    "# print(\"number of docs:\",len(embeddings))\n",
    "# print(\"dimension of docs:\",len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 3009\n",
      "dimension of docs: 384\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = embed_model.embed_documents(data['text'])\n",
    "print(\"number of docs:\",len(embeddings))\n",
    "print(\"dimension of docs:\",len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize Pinecone\n",
    "pc = Pinecone(api_key=os.environ.get('PINECONE_API_KEY'))\n",
    "index_name = 'medical-articles-embeddings'\n",
    "#initialize the index\n",
    "pc.create_index(\n",
    "    index_name,\n",
    "    dimension=384,#len(embeddings[0]),\n",
    "    metric='cosine',\n",
    "    spec= PodSpec(environment=\"gcp-starter\")\n",
    ")\n",
    "# Describe the index\n",
    "index_name = 'medical-articles-embeddings'\n",
    "index = pc.Index(index_name)\n",
    "index.describe_index_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# batch_size = 32\n",
    "\n",
    "# for i in tqdm(range(0, len(data), batch_size)):\n",
    "#     i_end = min(len(data), i+batch_size)\n",
    "#     batch = data.iloc[i:i_end]\n",
    "#     ids = [f\"{x['id']}\" for _, x in batch.iterrows()]\n",
    "#     texts = [x['text'] for _, x in batch.iterrows()]\n",
    "#     inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#     with torch.no_grad():\n",
    "#         outputs = embedding_model(**inputs)\n",
    "#         encoded_abstracts.append(outputs.last_hidden_state.mean(dim=1))\n",
    "#     embeds= torch.cat(encoded_abstracts, dim=0)\n",
    "\n",
    "\n",
    "#     # get metadata to store in Pinecone\n",
    "#     metadata = [\n",
    "#         {'text': x['text'],\n",
    "#          'source': x['source']} for _, x in batch.iterrows()\n",
    "#     ]\n",
    "#     index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2931/2931 [32:34<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    batch = data.iloc[i:i_end]\n",
    "    ids = [f\"{x['id']}\" for _, x in batch.iterrows()]\n",
    "    texts = [x['text'] for _, x in batch.iterrows()]\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['text'],\n",
    "         'source': x['source']} for _, x in batch.iterrows()\n",
    "    ]\n",
    "    # metadata = [\n",
    "    #     {'text': x['text']} for _, x in batch.iterrows()\n",
    "    # ]\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 384,\n",
       " 'index_fullness': 0.93776,\n",
       " 'namespaces': {'': {'vector_count': 93776}},\n",
       " 'total_vector_count': 93776}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Describe the index\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Answering Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34: Matthew R|Levy, Joshua J,, Data of Publication: 2023 Oct 21, Terms or keywords associated with the article: Mohs micrographic surgery|artificial intelligence|clinical research|general dermatology|medical dermatology|oncology, Journal Title: Experimental dermatology,\n",
      "0.29: PMID: 35104814, Abstract: INTRODUCTION: Moyamoya disease is characterized by progressive stenotic changes in the terminal segment of the internal carotid artery and the development ofabnormal vascular networks called moyamoya vessels. The objective of this reviewwas to provide a holistic view of the epidemiology, etiology, clinical findings,treatment, and pathogenesis of moyamoya disease. A literature search wasperformed in PubMed using the term \"\"moyamoya disease,\"\" for articles publisheduntil 2021. RESULTS: Artificial intelligence (AI) clustering was used to classifythe articles into 5 clusters: (1) pathophysiology (23.5%); (2) clinicalbackground (37.3%); (3) imaging (13.2%); (4) treatment (17.3%); and (5) genetics(8.7%). Many articles in the \"\"clinical background\"\" cluster were published fromthe 1970s. However, in the \"\"treatment\"\" and \"\"genetics\"\" clusters, the articles werepublished from the 2010s through 2021. In 2011, it was confirmed that a genecalled Ringin protein 213 (RNF213) is a susceptibility gene for moyamoya disease.Since then, tremendous progress in genomic, transcriptomic, and epigeneticprofiling (e.g., methylation profiling) has resulted in new concepts forclassifying moyamoya disease. Our literature survey revealed that thepathogenesis involves aberrations of multiple signaling pathways through geneticmutations and altered gene expression. CONCLUSION: We analyzed the contentvectors in abstracts using AI, and reviewed the pathophysiology, clinicalbackground, radiological features, treatments, and genetic peculiarity ofmoyamoya disease., Title: Macrohistory of Moyamoya Disease Analyzed Using Artificial Intelligence., Authors: Kuribara, Tomoyoshi|Akiyama, Yukinori|Mikami, Takeshi|Komatsu, Katsuya|Kimura, Yusuke|Takahashi, Yasuhiro|Sakashita, Kyoya|Chiba, Ryohei|Mikuni,\n",
      "0.29: accuracy|Moyamoya disease, Journal Title: Journal of stroke and cerebrovascular diseases : the official journal of National Stroke Association, Medical subject headings: Adolescent|Adult|*Deep Learning|*Diagnosis, Computer-Assisted|Diagnosis, Differential|Female|Humans|*Image Interpretation, Computer-Assisted|Intracranial Arteriosclerosis/*diagnostic imaging|*Magnetic Resonance Imaging|Male|Middle Aged|Moyamoya Disease/*diagnostic imaging|Predictive Value of Tests|Reproducibility of Results|Retrospective Studies|Young Adult,\n",
      "0.29: Dysfunction in Adults With Moyamoya Disease: Retrospective Cohort Analysis., Authors: Tsunoda, Sho|Inoue, Tomohiro|Ohwaki, Kazuhiro|Takeuchi, Naoko|Shinkai, Takako|Fukuda, Akira|Segawa, Masafumi|Kawashima, Mariko|Akabane, Atsuya|Miyawaki, Satoru|Saito, Nobuhito,, Data of Publication: 2023 Mar 1, Journal Title: Neurosurgery, Medical subject headings: Humans|Adult|*Moyamoya Disease/complications/diagnostic imaging|Retrospective Studies|*Cognitive Dysfunction/etiology|Frontal Lobe/diagnostic imaging|Hemodynamics,\n",
      "0.28: PMID: 27611897, Abstract: OBJECTIVE Majewski osteodysplastic primordial dwarfism Type II (MOPD II) is a rare genetic disorder. Features of it include extremely small stature, severemicrocephaly, and normal or near-normal intelligence. Previous studies have foundthat more than 50% of patients with MOPD II have intracranial vascular anomalies,but few successful surgical revascularization or aneurysm-clipping cases havebeen reported because of the diminutive arteries and narrow surgical corridors inthese patients. Here, the authors report on a large series of patients with MOPDII who underwent surgery for an intracranial vascular anomaly. METHODS Inconjunction with an approved prospective registry of patients with MOPD II, aprospectively collected institutional surgical database of children with MOPD IIand intracranial vascular anomalies who underwent surgery was analyzedretrospectively to establish long-term outcomes. RESULTS Ten patients with MOPDII underwent surgery between 2005 and 2012; 5 patients had moyamoya disease(MMD), 2 had intracranial aneurysms, and 3 had both MMD and aneurysms. Patientspresented with transient ischemic attack (TIA) (n = 2), ischemic stroke (n = 2),intraparenchymal hemorrhage from MMD (n = 1), and aneurysmal subarachnoidhemorrhage (n = 1), and 4 were diagnosed on screening. The mean age of the 8patients with MMD, all of whom underwent extracranial-intracranialrevascularization (14 indirect, 1 direct) was 9 years (range 1-17 years). Themean age of the 5 patients with aneurysms was 15.5 years (range 9-18 years). Twopatients experienced postoperative complications (1 transient weakness afterclipping, 1 femoral thrombosis that required surgical repair). During a\n"
     ]
    }
   ],
   "source": [
    "# Check the scores for the top 5 matches\n",
    "query = 'who is Moog'\n",
    "\n",
    "# query\n",
    "results = index.query(vector=embed_model.embed_query(query), top_k=5, include_metadata=True)\n",
    "for result in results['matches']:\n",
    "    print(f\"{round(result['score'], 2)}: {result['metadata']['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the top N results\n",
    "from langchain.vectorstores import Pinecone\n",
    "vectorstore = Pinecone(index, embed_model.embed_query, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='for a CASK pathogenic variant and in a male who is hemizygousfor a CASK pathogenic variant on molecular genetic testing. Rarely, affectedmales have a mosaic pathogenic variant. MANAGEMENT: Treatment of manifestations:Treatment is symptomatic and includes standard management of developmental delayand intellectual disability issues; medication for seizures; nutritional support;use of physiotherapy; and treatment of abnormal vision or hearing loss. GENETICCOUNSELING: CASK disorders are inherited in an X-linked manner. Risk to thefamily members of a proband with a CASK disorder depends on'),\n",
       " Document(page_content='thefamily members of a proband with a CASK disorder depends on the phenotype (i.e.,MICPCH or XLID +/- nystagmus) in the proband. MICPCH. Most affected females andmales represent simplex cases (i.e., the only affected family member) and havethe disorder as the result of a de novo CASK pathogenic variant. Becauseheterozygous females manifest the phenotype, an asymptomatic mother is unlikelyto be heterozygous for the CASK pathogenic variant. If a proband represents asimplex case, the recurrence risk to sibs appears to be low but greater'),\n",
       " Document(page_content='case, the recurrence risk to sibs appears to be low but greater than thatof the general population because of the possibility of parental germlinemosaicism. XLID +/- nystagmus. The father of a male with a CASK disorder will nothave the disorder nor will he be hemizygous for the CASK pathogenic variant. If amale is the only affected family member, the mother may be a heterozygote or theaffected male may have a de novo pathogenic variant. In a family with more thanone affected individual, the mother of an affected male')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = 'what is the cause of CASK Disorders?'\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # the search query\n",
    "    k=3  # returns top 3 most relevant chunks of text\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
