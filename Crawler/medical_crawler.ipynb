{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO NEED TO RUN THIS CELL AS THE DATA IS ALREADY COLLECTED AND SAVE UNDER articles.csv\n",
    "\n",
    "# Data Crawler that works exactly like a human and go one by one through the articles and save the abstracts and references in XML format\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Navigate to the website\n",
    "url = \"https://pubmed.ncbi.nlm.nih.gov/?term=intelligence+%5BTitle%2Fabstract%5D&filter=simsearch1.fha&filter=years.2013-2023&sort=date&size=200\"\n",
    "driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()))\n",
    "driver.get(url)\n",
    "\n",
    "# Click the first article to start\n",
    "driver.find_element(By.XPATH, \"//a[@data-ga-action=1]\").click()\n",
    "total_articles = driver.find_element(By.XPATH, \"//*[@id='adjacent-navigation']/div[2]/a/span[1]/span[2]\").text\n",
    "\n",
    "# Find the number of total articles\n",
    "total_articles = re.sub(r\"[^3-9]\",'', total_articles)\n",
    "\n",
    "# Specify the chunk size\n",
    "chunk_size = 1000\n",
    "\n",
    "# Create empty lists to store data\n",
    "titles = []\n",
    "authors = []\n",
    "abstracts = []\n",
    "references_list = []\n",
    "not_found_pages = []\n",
    "\n",
    "for page in tqdm(range(int(total_articles))):\n",
    "    # Extract title of the article\n",
    "    try:\n",
    "        title = driver.find_element(By.CLASS_NAME, \"heading-title\")\n",
    "        if title.is_displayed():\n",
    "            title = title.text\n",
    "    except NoSuchElementException:\n",
    "        title = ''\n",
    "        pass\n",
    "\n",
    "    # Extract autors of the article\n",
    "    try:\n",
    "        authors_elements = driver.find_elements(By.CLASS_NAME, \"full-name\")\n",
    "        author_list = []\n",
    "        if len(authors_elements) > 0:\n",
    "            for author in authors_elements:\n",
    "                author_list.append(author.text)\n",
    "    except NoSuchElementException:\n",
    "        author_list.append('')\n",
    "        pass\n",
    "\n",
    "    # Extract abstract of the article\n",
    "    try:\n",
    "        abstract = driver.find_element(By.ID, \"eng-abstract\")\n",
    "        if abstract.is_displayed():\n",
    "            abstract = abstract.text\n",
    "    except NoSuchElementException:\n",
    "        abstract = ''\n",
    "        pass\n",
    "\n",
    "    # Check and extract if there is reference or are more references \n",
    "    try:\n",
    "        reference = driver.find_element(By.ID, \"references\")\n",
    "        show_all_element = driver.find_element(By.CLASS_NAME, \"show-all\")\n",
    "        if show_all_element.is_displayed():\n",
    "            show_all_element.click()\n",
    "        if reference.is_displayed():\n",
    "            references = driver.find_element(By.CLASS_NAME, \"references-list\").text\n",
    "    except NoSuchElementException:\n",
    "        references = ''\n",
    "        pass\n",
    "\n",
    "    # Append data to lists\n",
    "    titles.append(title)\n",
    "    authors.append(author_list)\n",
    "    abstracts.append(abstract)\n",
    "    references_list.append(references)\n",
    "\n",
    "    if (page + 1) % chunk_size == 0 or page + 1 == int(total_articles):\n",
    "        # Create a DataFrame\n",
    "        data = {\n",
    "                    'Title': pd.Series(titles),\n",
    "                    'Authors': pd.Series(authors),\n",
    "                    'Abstracts': pd.Series(abstracts),\n",
    "                    'References': pd.Series(references_list)\n",
    "                }\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        chunk_number = (page + 1) // chunk_size\n",
    "        csv_filename = f'pubmed_data_chunk_{chunk_number}.csv'\n",
    "        df.to_csv(csv_filename, index=False)\n",
    "\n",
    "        # Clear lists for the next chunk\n",
    "        titles = []\n",
    "        authors = []\n",
    "        abstracts = []\n",
    "        references_list = []\n",
    "    \n",
    "    # Navigate to the next article\n",
    "    try:\n",
    "        next_page = driver.find_element(By.XPATH, \"//div[@class='next side-link visible']\")\n",
    "        if next_page.is_displayed():\n",
    "            next_page.click()\n",
    "    except NoSuchElementException:\n",
    "        not_found_pages.append(page)\n",
    "        print(f\"{page = } not found!\")\n",
    "        pass\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing to have a CSV file with the following columns and respective data in each row:\n",
    "# which will be filtered later to have just useful columns and non dupplicated PMID\n",
    "\n",
    "\"\"\"\n",
    "df shape before cleaning:(74243, 77)\n",
    "df shape after cleaning:(57560, 15)\n",
    "\n",
    "Index(['PMID', 'STAT', 'DRDT', 'CTDT', 'PB', 'DP', 'TI', 'BTI', 'AB', 'CI',\n",
    "       'FED', 'ED', 'FAU', 'AU', 'AD', 'LA', 'PT', 'PL', 'OTO', 'OT', 'EDAT',\n",
    "       'CRDT', 'AID', 'OWN', 'DCOM', 'LR', 'IS', 'VI', 'IP', 'PG', 'LID',\n",
    "       'DEP', 'TA', 'JT', 'JID', 'SB', 'MH', 'MHDA', 'PHST', 'PST', 'SO', 'GR',\n",
    "       'PMC', 'MID', 'COIS', 'TT', 'RN', 'OID', 'SI', 'ISBN', 'CTI', 'CN',\n",
    "       'FIR', 'IR', 'AUID', 'EIN', 'CIN', 'PS', 'FPS', 'CON', 'UOF', 'UIN',\n",
    "       'RIN', 'IRAD', 'EFR', 'OAB', 'OABL', 'PMCR', 'CP', 'ECI', 'DRIN', 'RF',\n",
    "       'EN', 'ROF', 'RPI', 'RPF', 'DDIN'],\n",
    "      dtype='object')\n",
    "\n",
    "\n",
    "Guide to the abbreviations:\n",
    "\n",
    "    PMID: PubMed IDentifier - Unique identifier for a PubMed record.\n",
    "\n",
    "    TI: Title - The title of the article.\n",
    "\n",
    "    AB: Abstract - A brief summary of the article's content.\n",
    "\n",
    "    PB: Publisher - The organization responsible for publishing the article.\n",
    "\n",
    "    FAU: Full Author(s) - Full names of the authors.\n",
    "\n",
    "    FED: Full Editor(s) - Full names of the editors.\n",
    "\n",
    "    DP: Date of Publication - Date when the article was published.\n",
    "\n",
    "    OTO: Other Term Owner - Owner of other terms.\n",
    "\n",
    "    OT: Other Term - Additional terms or keywords associated with the article.\n",
    "\n",
    "    OWN: Owner - Owner of the article.\n",
    "\n",
    "    DCOM: Date Completed - Date when the article was completed.\n",
    "\n",
    "    LR: Last Revision - Last revision date.\n",
    "\n",
    "    JT: Journal Title - Full title of the journal.\n",
    "\n",
    "    MH: MeSH Terms - Medical Subject Headings.\n",
    "\n",
    "    ISBN: International Standard Book Number - ISBN of the article.\n",
    "\n",
    "[Removed]    STAT: Status - Indicates the status of the publication.\n",
    "\n",
    "[Removed]    DRDT: Date Received by Database Transfer - Date when the record was received by the database.\n",
    "\n",
    "[Removed]    CTDT: Current Temporary Date - Current temporary date of the record.\n",
    "\n",
    "[Removed]    BTI: Book Title Indicator - Indicates that the article is part of a book.\n",
    "\n",
    "[Removed]    CI: Copyright Information - Information about the copyright holder.\n",
    "\n",
    "[Removed]    ED: Editor - Abbreviation for the editor.\n",
    "\n",
    "[Removed]    AU: Author - Abbreviation for the author.\n",
    "\n",
    "[Removed]    AD: Author's Affiliation - Affiliation or institution of the author.\n",
    "\n",
    "[Removed]    LA: Language - Language of the article.\n",
    "\n",
    "[Removed]    PT: Publication Type - Type of publication (e.g., Review, Book Chapter).\n",
    "\n",
    "[Removed]    PL: Place of Publication - Location where the article was published.\n",
    "\n",
    "[Removed]    EDAT: Entrez Date - Date the record was added to the Entrez database.\n",
    "\n",
    "[Removed]    CRDT: Create Date - Date the record was created.\n",
    "\n",
    "[Removed]    AID: Article Identifier - Identifier associated with the article.\n",
    "\n",
    "[Removed]    IS: Issue - Issue number of the journal.\n",
    "\n",
    "[Removed]    VI: Volume - Volume number of the journal.\n",
    "\n",
    "[Removed]    IP: Issue Part - Part number of the issue.\n",
    "\n",
    "[Removed]    PG: Page - Page number.\n",
    "\n",
    "[Removed]    LID: Location IDentifier - Identifier for the location of the article.\n",
    "\n",
    "[Removed]    DEP: Date of Electronic Publication - Date of electronic publication.\n",
    "\n",
    "[Removed]    TA: Journal Title (ISO abbreviation) - Title abbreviation of the journal.\n",
    "\n",
    "[Removed]    JID: Journal ID - Identifier for the journal.\n",
    "\n",
    "[Removed]    SB: Subset - Subset designation.\n",
    "\n",
    "[Removed]    MHDA: MeSH Date - MeSH date.\n",
    "\n",
    "[Removed]    PHST: Publication History Status - Publication history status.\n",
    "\n",
    "[Removed]    PST: Publication Status - Publication status.\n",
    "\n",
    "[Removed]    SO: Source - Source of the article.\n",
    "\n",
    "[Removed]    GR: Grant - Grant information.\n",
    "\n",
    "[Removed]    PMC: PubMed Central ID - Identifier for PubMed Central.\n",
    "\n",
    "[Removed]    MID: Manuscript ID - Identifier for the manuscript.\n",
    "\n",
    "[Removed]    COIS: Conflict of Interest Statement - Statement about potential conflicts of interest.\n",
    "\n",
    "[Removed]    TT: Type of Test - Type of test.\n",
    "\n",
    "[Removed]    RN: Registry Number - Registry number.\n",
    "\n",
    "[Removed]    OID: Organization ID - Identifier for the organization.\n",
    "\n",
    "[Removed]    SI: Secondary Source ID - Secondary source identifier.\n",
    "\n",
    "[Removed]    CTI: Current Technology Information - Current technology information.\n",
    "\n",
    "[Removed]    CN: Contract Number - Contract number.\n",
    "\n",
    "[Removed]    FIR: Full Investigator(s) - Full names of the investigators.\n",
    "\n",
    "[Removed]    IR: Investigator - Abbreviation for the investigator.\n",
    "\n",
    "[Removed]    AUID: Author ID - Identifier for the author.\n",
    "\n",
    "[Removed]    EIN: Editor's ID - Identifier for the editor.\n",
    "\n",
    "[Removed]    CIN: Contributor ID - Identifier for the contributor.\n",
    "\n",
    "[Removed]    PS: Personal Name as Subject - Personal name as subject.\n",
    "\n",
    "[Removed]    FPS: Full Personal Name as Subject - Full personal name as subject.\n",
    "\n",
    "[Removed]    CON: Consortium - Consortium information.\n",
    "\n",
    "[Removed]    UOF: Use of Funds - Use of funds information.\n",
    "\n",
    "[Removed]    UIN: Unique Identifier - Unique identifier.\n",
    "\n",
    "[Removed]    RIN: Reviewer ID - Reviewer identifier.\n",
    "\n",
    "[Removed]    IRAD: Investigator Affiliation Department - Investigator affiliation department.\n",
    "\n",
    "[Removed]    EFR: EFS (Endoscopic Frequency Standardization) Factor - EFS factor.\n",
    "\n",
    "[Removed]    OAB: Overall Bank - Overall bank.\n",
    "\n",
    "[Removed]    OABL: Overall Blood - Overall blood.\n",
    "\n",
    "[Removed]    PMCR: PubMed Central Release - PubMed Central release information.\n",
    "\n",
    "[Removed]    CP: Clinical Progress - Clinical progress.\n",
    "\n",
    "[Removed]    ECI: Early Career Investigator - Early career investigator.\n",
    "\n",
    "[Removed]    DRIN: Dual Purpose Experimental Purpose Indicator - Dual-purpose experimental purpose indicator.\n",
    "\n",
    "[Removed]    RF: Release Factor - Release factor.\n",
    "\n",
    "[Removed]    EN: Endorsement - Endorsement.\n",
    "\n",
    "[Removed]    ROF: Reviewer's Office - Reviewer's office.\n",
    "\n",
    "[Removed]    RPI: Reviewer's Position Identifier - Reviewer's position identifier.\n",
    "\n",
    "[Removed]    RPF: Research Performance Factor - Research performance factor.\n",
    "\n",
    "[Removed]    DDIN: Degree-Degree Integration Network - Degree-degree integration network.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "with open('../crawler/articles.txt', 'r', encoding='utf-8') as f:\n",
    "    input_text = f.read()\n",
    "\n",
    "# Split articles based on double quotes\n",
    "articles = re.split(r'\\n\"\\n', input_text.strip())\n",
    "\n",
    "# Define a function to extract data from each article\n",
    "def extract_data(article):\n",
    "    data = {}\n",
    "    current_key = None\n",
    "    current_value = ''\n",
    "\n",
    "    for line in article.split('\\n'):\n",
    "        # matching the key-value pair\n",
    "        match = re.match(r'^([A-Z]{2,4})\\s*- (.+)$', line)\n",
    "\n",
    "        if match:\n",
    "            key, value = match.groups()\n",
    "            if current_key:\n",
    "                # If a key is already set, save the current value\n",
    "                if current_key in data:\n",
    "                    data[current_key] += '|' + current_value\n",
    "                else:\n",
    "                    data[current_key] =  current_value.strip()\n",
    "                current_value = ''  # Reset current value\n",
    "\n",
    "            current_key = key\n",
    "            current_value = value\n",
    "        else:\n",
    "            # If there's no match, append the line to the current value\n",
    "            current_value += '' + line.strip()\n",
    "\n",
    "    # Save the last key-value pair\n",
    "    if current_key:\n",
    "        data[current_key] = current_value.strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Extract data from each article\n",
    "article_data_list = [extract_data(article) for article in articles]\n",
    "\n",
    "# Filter out articles without 'AB' key\n",
    "filtered_data_list = [data for data in article_data_list if 'AB' in data]\n",
    "\n",
    "# Create a DataFrame from the filtered data\n",
    "df = pd.DataFrame(filtered_data_list)\n",
    "\n",
    "# Keep only useful columns:  df shape before cleaning:(74243, 77)\n",
    "df = df[['PMID', 'TI', 'AB', 'PB', 'FAU', 'FED', 'DP', 'OTO', 'OT', 'OWN', 'DCOM', 'LR', 'JT', 'MH', 'ISBN']]\n",
    "\n",
    "# Drop duplicates based on the 'PMID' column : df shape after cleaning:(57560, 15)\n",
    "df = df.drop_duplicates(subset='PMID', keep='first')\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../crawler/articles.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic TF-IDF approach and the pandas library for data manipulation:\n",
    "# which will search for the top 5 most similar articles to a given query.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Load your CSV data\n",
    "data = pd.read_csv('articles.csv')\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['AB'].fillna(''))\n",
    "\n",
    "# Function to search for queries\n",
    "def search(query, tfidf_matrix, data):\n",
    "    query_vector = tfidf_vectorizer.transform([query])\n",
    "    cosine_similarities = linear_kernel(query_vector, tfidf_matrix).flatten()\n",
    "    document_scores = list(enumerate(cosine_similarities))\n",
    "    document_scores = sorted(document_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the top N results\n",
    "    top_results = document_scores[:5]\n",
    "    for idx, score in top_results:\n",
    "        print(f\"Title: {data['TI'].iloc[idx]}, Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "search(\"IQ scores\", tfidf_matrix, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
