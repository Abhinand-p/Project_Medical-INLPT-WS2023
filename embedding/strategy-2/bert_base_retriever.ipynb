{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Transformer for Medical Text QA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch\n",
    "\n",
    "# Load dataset \n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Path to store precomputed embeddings\n",
    "embeddings_path = 'precomputed_embeddings.npy'\n",
    "\n",
    "# Set up NLTK stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess and embed text in batches\n",
    "def preprocess_and_embed_batch(texts, batch_size=32):\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings in batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = preprocess_and_embed(batch_texts)\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    return np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "# Function to preprocess and embed text\n",
    "def preprocess_and_embed(texts):\n",
    "    # Remove stop words and apply stemming\n",
    "    processed_texts = [' '.join([stemmer.stem(token) for token in tokenizer.tokenize(text) if token not in stop_words]) for text in texts]\n",
    "\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(processed_texts, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():  # This block ensures GPU usage\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the embeddings for the [CLS] token\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def compute_and_save_embeddings(df, embeddings_path, incremental=False):\n",
    "    if incremental and os.path.exists(embeddings_path):\n",
    "        # Load existing embeddings\n",
    "        existing_embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "        # Identify new abstracts to process\n",
    "        new_abstracts = df.loc[~df.index.isin(existing_embeddings.index), 'AB']\n",
    "\n",
    "        if not new_abstracts.empty:\n",
    "            new_embeddings = preprocess_and_embed_batch(new_abstracts)\n",
    "            updated_embeddings = np.concatenate([existing_embeddings, new_embeddings], axis=0)\n",
    "        else:\n",
    "            # Nothing new to process\n",
    "            updated_embeddings = existing_embeddings\n",
    "\n",
    "    else:\n",
    "        # Process all abstracts\n",
    "        updated_embeddings = preprocess_and_embed_batch(df['AB'])\n",
    "\n",
    "    # Save updated embeddings\n",
    "    np.save(embeddings_path, updated_embeddings)\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    return np.load(embeddings_path)\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        if any(token in sentence_tokens for token in query_tokens):\n",
    "            return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, embeddings, num_results=5):\n",
    "    # Preprocess the query\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Calculate embeddings for the query\n",
    "    query_embedding = preprocess_and_embed(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = [cosine_similarity(query_embedding, ae.reshape(1, -1))[0][0] for ae in tqdm(embeddings, desc=\"Calculating similarities\")]\n",
    "    \n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:num_results]\n",
    "    \n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Check if precomputed embeddings exist, otherwise compute and save them\n",
    "if not os.path.exists(embeddings_path):\n",
    "    compute_and_save_embeddings(df, embeddings_path)\n",
    "\n",
    "# Load precomputed embeddings\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without misspelling \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding the abstracts using BERT and saving them to a file\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the abstracts\n",
    "def encode_abstracts_sliding_window(abstracts, window_size=512, stride=256):\n",
    "    encoded_abstracts = []\n",
    "\n",
    "    for abstract in tqdm(abstracts, desc=\"Encoding Abstracts\", unit=\"abstract\"):\n",
    "        tokens = tokenizer.tokenize(abstract)\n",
    "        total_length = len(tokens)\n",
    "\n",
    "        # Determine the number of overlapping windows\n",
    "        num_windows = abs(total_length - window_size) // stride + 1\n",
    "\n",
    "        for i in range(0, num_windows * stride, stride):\n",
    "            # Extract a window of tokens\n",
    "            window_tokens = tokens[i:i + window_size]\n",
    "\n",
    "            # Convert tokens back to a string\n",
    "            window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "\n",
    "            # Tokenize and encode the window\n",
    "            inputs = tokenizer(window_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            encoded_abstracts.append(outputs.last_hidden_state.mean(dim=1))\n",
    "\n",
    "    if not encoded_abstracts:\n",
    "        print(\"No encoded abstracts found.\")\n",
    "    return torch.cat(encoded_abstracts, dim=0)\n",
    "\n",
    "\n",
    "# Function to save encoded abstracts\n",
    "def save_encoded_abstracts(encoded_abstracts, filename):\n",
    "    torch.save(encoded_abstracts, filename)\n",
    "\n",
    "# Function to load encoded abstracts\n",
    "def load_encoded_abstracts(filename):\n",
    "    return torch.load(filename)\n",
    "\n",
    "# Example: Load, encode, and save each part separately\n",
    "for i in tqdm(range(1, 11), desc=\"Processing Parts\", unit=\"part\"):\n",
    "    file_path = f'sub_data_{i}.csv'\n",
    "    df_part = pd.read_csv(file_path)\n",
    "\n",
    "    # Encode abstracts\n",
    "    encoded_abstracts_part = encode_abstracts_sliding_window(df_part['Combined_Info'])\n",
    "\n",
    "    # Save encoded abstracts\n",
    "    save_encoded_abstracts(encoded_abstracts_part, f'encoded_data_part_{i}.pt')\n",
    "\n",
    "# Load and concatenate encoded abstracts from all parts\n",
    "encoded_abstracts_parts = []\n",
    "for i in tqdm(range(1, 11), desc=\"Loading Parts\", unit=\"part\"):\n",
    "    encoded_abstracts_part = load_encoded_abstracts(f'encoded_data_part_{i}.pt')\n",
    "    encoded_abstracts_parts.append(encoded_abstracts_part)\n",
    "\n",
    "# Concatenate the parts\n",
    "encoded_abstracts = torch.cat(encoded_abstracts_parts, dim=0)\n",
    "\n",
    "# Save the encoded_abstracts tensor\n",
    "torch.save(encoded_abstracts, 'encoded_data.pt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"retrieving the most similar abstracts to a question and then generating an answer based on those abstracts is a \n",
    "reasonable strategy for a Question Answering (QA) system. While the SQuAD (Stanford Question Answering Dataset) \n",
    "is typically used for training and evaluating QA models, you can adapt your approach to leverage \n",
    "the idea of retrieving relevant passages or abstracts and then generating answers.\n",
    "\n",
    "Steps:\n",
    "\n",
    "    1-Retrieve Similar Abstracts:\n",
    "        Use a method (such as cosine similarity) to retrieve the top N most similar abstracts to a given question from your collection of abstracts.\n",
    "\n",
    "    2-Generate Answers:\n",
    "        For each of the retrieved abstracts, use a QA model to generate answers to the question.\n",
    "        Fine-tune a pre-trained QA model on your specific dataset, considering the structure of your abstracts and questions.\n",
    "\n",
    "    3-Combine Answers:\n",
    "        Aggregate or combine the answers generated from different abstracts to provide a final answer.\"\"\"\n",
    "\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "qa_model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "qa_tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "\n",
    "# Tokenize and encode the abstracts\n",
    "def encode_abstracts_sliding_window(abstracts, window_size=512, stride=256):\n",
    "    encoded_abstracts = []\n",
    "\n",
    "    for abstract in tqdm(abstracts, desc=\"Encoding Abstracts\", unit=\"abstract\"):\n",
    "        tokens = tokenizer.tokenize(abstract)\n",
    "        total_length = len(tokens)\n",
    "\n",
    "        # Determine the number of overlapping windows\n",
    "        num_windows = abs(total_length - window_size) // stride + 1\n",
    "\n",
    "        for i in range(0, num_windows * stride, stride):\n",
    "            # Extract a window of tokens\n",
    "            window_tokens = tokens[i:i + window_size]\n",
    "\n",
    "            # Convert tokens back to a string\n",
    "            window_text = tokenizer.convert_tokens_to_string(window_tokens)\n",
    "\n",
    "            # Tokenize and encode the window\n",
    "            inputs = tokenizer(window_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            encoded_abstracts.append(outputs.last_hidden_state.mean(dim=1))\n",
    "\n",
    "    if not encoded_abstracts:\n",
    "        print(\"No encoded abstracts found.\")\n",
    "    return torch.cat(encoded_abstracts, dim=0)\n",
    "\n",
    "\n",
    "# Function to load encoded abstracts\n",
    "def load_encoded_abstracts(filename):\n",
    "    return torch.load(filename)\n",
    "\n",
    "# Function to retrieve top k similar abstracts\n",
    "def retrieve_top_k_abstracts(query, abstracts, df, k=5):\n",
    "    # Encode the query using the sliding window approach (as before)\n",
    "    query_embedding = encode_abstracts_sliding_window([query])\n",
    "    \n",
    "    # Calculate cosine similarity between the query and encoded abstracts\n",
    "    similarities = cosine_similarity(query_embedding, abstracts)\n",
    "    \n",
    "    # Get the indices of the top k most similar abstracts\n",
    "    top_k_indices = similarities.argsort()[0, -k:][::-1]\n",
    "\n",
    "    if len(top_k_indices) == 0:\n",
    "        print(\"No matching abstracts found.\")\n",
    "        return []\n",
    "\n",
    "    # Print some information for debugging\n",
    "    print(\"Top k PMIDs:\", df.index[top_k_indices].tolist())\n",
    "    print(\"Abstract lengths:\", [len(df.loc[pmid, 'Combined_Info']) for pmid in df.index[top_k_indices]])\n",
    "\n",
    "    return top_k_indices\n",
    "\n",
    "\n",
    "# Function to generate answers using the QA model\n",
    "def generate_answers(question, abstracts, df):\n",
    "    answers = []\n",
    "\n",
    "    for index in abstracts:\n",
    "        # Get the PMID\n",
    "        pmid = df.index[index]\n",
    "\n",
    "        # Get the abstract text\n",
    "        abstract_text = df.loc[pmid, 'Combined_Info']\n",
    "\n",
    "        # Tokenize and encode the question and abstract\n",
    "        inputs = qa_tokenizer(question, abstract_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "        \n",
    "        # Perform inference with the QA model\n",
    "        with torch.no_grad():\n",
    "            outputs = qa_model(**inputs)\n",
    "\n",
    "        # Get the predicted answer\n",
    "        answer_start = torch.argmax(outputs.start_logits)\n",
    "        answer_end = torch.argmax(outputs.end_logits) + 1\n",
    "        answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "\n",
    "        answers.append(answer)\n",
    "\n",
    "    return answers\n",
    "\n",
    "# Using PMID as the index column\n",
    "df_part = pd.read_csv('data_1.csv', index_col='PMID')\n",
    "# df_part = pd.read_csv('articles.csv', index_col='PMID', usecols=['TI', 'AB', 'FAU', 'DP', 'OT', 'JT', 'MH'])\n",
    "\n",
    "\n",
    "# Example usage\n",
    "encoded_abstracts = load_encoded_abstracts('encoded_data.pt')\n",
    "question = \"what is Artificial Intelligence?\"\n",
    "top_k_abstracts = retrieve_top_k_abstracts(question, encoded_abstracts, df_part, k=5)\n",
    "\n",
    "# Print the top 5 similar abstracts\n",
    "print(\"Top 5 Similar Abstracts:\")\n",
    "for index in top_k_abstracts:\n",
    "    pmid = df_part.index[index]\n",
    "    print(\"PMID:\", pmid)\n",
    "    print(\"Abstract:\", df_part.loc[pmid, 'Combined_Info'])\n",
    "\n",
    "answers = generate_answers(question, top_k_abstracts, df_part)\n",
    "\n",
    "# Display the generated answers\n",
    "print(\"\\nGenerated Answers:\")\n",
    "for answer in answers:\n",
    "    print(answer)\n",
    "#[TODO] check if the retrieved data is correct as sometimes it produces some IDs that are not part of the dataset, e.g. query = \"who is Chenq?\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
