{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Transformer for Medical Text QA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import torch\n",
    "\n",
    "# Load dataset \n",
    "dataset_path = 'articles.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Preprocess the dataset\n",
    "df['AB'] = df['AB'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Path to store precomputed embeddings\n",
    "embeddings_path = 'precomputed_embeddings.npy'\n",
    "\n",
    "# Set up NLTK stopwords and stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to preprocess and embed text in batches\n",
    "def preprocess_and_embed_batch(texts, batch_size=32):\n",
    "    embeddings_list = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Computing embeddings in batches\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        batch_embeddings = preprocess_and_embed(batch_texts)\n",
    "        embeddings_list.append(batch_embeddings)\n",
    "\n",
    "    return np.concatenate(embeddings_list, axis=0)\n",
    "\n",
    "# Function to preprocess and embed text\n",
    "def preprocess_and_embed(texts):\n",
    "    # Remove stop words and apply stemming\n",
    "    processed_texts = [' '.join([stemmer.stem(token) for token in tokenizer.tokenize(text) if token not in stop_words]) for text in texts]\n",
    "\n",
    "    # Tokenize and encode the input text\n",
    "    inputs = tokenizer(processed_texts, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "\n",
    "    # Forward pass through the BERT model\n",
    "    with torch.no_grad():  # This block ensures GPU usage\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # Extract the embeddings for the [CLS] token\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].detach().cpu().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def compute_and_save_embeddings(df, embeddings_path, incremental=False):\n",
    "    if incremental and os.path.exists(embeddings_path):\n",
    "        # Load existing embeddings\n",
    "        existing_embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "        # Identify new abstracts to process\n",
    "        new_abstracts = df.loc[~df.index.isin(existing_embeddings.index), 'AB']\n",
    "\n",
    "        if not new_abstracts.empty:\n",
    "            new_embeddings = preprocess_and_embed_batch(new_abstracts)\n",
    "            updated_embeddings = np.concatenate([existing_embeddings, new_embeddings], axis=0)\n",
    "        else:\n",
    "            # Nothing new to process\n",
    "            updated_embeddings = existing_embeddings\n",
    "\n",
    "    else:\n",
    "        # Process all abstracts\n",
    "        updated_embeddings = preprocess_and_embed_batch(df['AB'])\n",
    "\n",
    "    # Save updated embeddings\n",
    "    np.save(embeddings_path, updated_embeddings)\n",
    "\n",
    "def load_embeddings(embeddings_path):\n",
    "    return np.load(embeddings_path)\n",
    "\n",
    "def extract_answer_sentence(query, abstract):\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', abstract)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        if any(token in sentence_tokens for token in query_tokens):\n",
    "            return sentence\n",
    "    \n",
    "    return None\n",
    "\n",
    "def search_engine(query, df, embeddings, num_results=5):\n",
    "    # Preprocess the query\n",
    "    query = query.lower()\n",
    "    \n",
    "    # Calculate embeddings for the query\n",
    "    query_embedding = preprocess_and_embed(query)\n",
    "    \n",
    "    # Calculate cosine similarity between the query and dataset abstracts\n",
    "    similarities = [cosine_similarity(query_embedding, ae.reshape(1, -1))[0][0] for ae in tqdm(embeddings, desc=\"Calculating similarities\")]\n",
    "    \n",
    "    # Get the indices of the top N most similar abstracts\n",
    "    top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:num_results]\n",
    "    \n",
    "    # Return the titles, scores, and answer sentences of the top N most relevant articles\n",
    "    top_articles = []\n",
    "    for i in top_indices:\n",
    "        title = df.iloc[i]['TI']\n",
    "        score = similarities[i]\n",
    "        abstract = df.iloc[i]['AB']\n",
    "        answer_sentence = extract_answer_sentence(query, abstract)\n",
    "        top_articles.append((title, score, answer_sentence))\n",
    "    \n",
    "    return top_articles\n",
    "\n",
    "# Check if precomputed embeddings exist, otherwise compute and save them\n",
    "if not os.path.exists(embeddings_path):\n",
    "    compute_and_save_embeddings(df, embeddings_path)\n",
    "\n",
    "# Load precomputed embeddings\n",
    "embeddings = load_embeddings(embeddings_path)\n",
    "\n",
    "# Example usage\n",
    "query = \"What is the treatment for cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with misspelling\n",
    "query = \"What is the treatent for lung caner?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")\n",
    "\n",
    "print(\"=\"*100)\n",
    "# without misspelling \n",
    "query = \"What is the treatment for lung cancer?\"\n",
    "top_articles = search_engine(query, df, embeddings)\n",
    "for title, score, answer_sentence in top_articles:\n",
    "    print(f\"Title: {title}, Score: {score}\")\n",
    "    print(f\"Answer Sentence: {answer_sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
