{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Installs</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uujCycZ265g6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pinecone-client in ./.local/lib/python3.9/site-packages (2.2.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pinecone-client) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pinecone-client) (6.0.1)\n",
      "Requirement already satisfied: loguru>=0.5.0 in ./.local/lib/python3.9/site-packages (from pinecone-client) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pinecone-client) (4.8.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in ./.local/lib/python3.9/site-packages (from pinecone-client) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pinecone-client) (2.0.6)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in ./.local/lib/python3.9/site-packages (from pinecone-client) (4.66.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pinecone-client) (1.26.0)\n",
      "Requirement already satisfied: six>=1.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests>=2.19.0->pinecone-client) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: faunadb in ./.local/lib/python3.9/site-packages (4.5.1)\n",
      "Requirement already satisfied: iso8601 in ./.local/lib/python3.9/site-packages (from faunadb) (2.1.0)\n",
      "Requirement already satisfied: requests in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from faunadb) (2.31.0)\n",
      "Requirement already satisfied: future in ./.local/lib/python3.9/site-packages (from faunadb) (0.18.3)\n",
      "Requirement already satisfied: httpx[http2] in ./.local/lib/python3.9/site-packages (from faunadb) (0.25.2)\n",
      "Requirement already satisfied: anyio in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from httpx[http2]->faunadb) (4.0.0)\n",
      "Requirement already satisfied: certifi in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from httpx[http2]->faunadb) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in ./.local/lib/python3.9/site-packages (from httpx[http2]->faunadb) (1.0.2)\n",
      "Requirement already satisfied: idna in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from httpx[http2]->faunadb) (3.4)\n",
      "Requirement already satisfied: sniffio in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from httpx[http2]->faunadb) (1.3.0)\n",
      "Requirement already satisfied: h2<5,>=3 in ./.local/lib/python3.9/site-packages (from httpx[http2]->faunadb) (4.1.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.local/lib/python3.9/site-packages (from httpcore==1.*->httpx[http2]->faunadb) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->faunadb) (3.3.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->faunadb) (2.0.6)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in ./.local/lib/python3.9/site-packages (from h2<5,>=3->httpx[http2]->faunadb) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in ./.local/lib/python3.9/site-packages (from h2<5,>=3->httpx[http2]->faunadb) (4.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from anyio->httpx[http2]->faunadb) (1.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ndg-httpsclient in ./.local/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: PyOpenSSL in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from ndg-httpsclient) (23.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from ndg-httpsclient) (0.5.0)\n",
      "Requirement already satisfied: cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from PyOpenSSL->ndg-httpsclient) (41.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->PyOpenSSL->ndg-httpsclient) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from cffi>=1.12->cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->PyOpenSSL->ndg-httpsclient) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyopenssl in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (23.2.0)\n",
      "Requirement already satisfied: cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from pyopenssl) (41.0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyopenssl) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from cffi>=1.12->cryptography!=40.0.0,!=40.0.1,<42,>=38.0.0->pyopenssl) (2.21)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyasn1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (0.5.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: sentence-transformers in ./.local/lib/python3.9/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (4.36.1)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.9/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (2.1.2)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.9/site-packages (from sentence-transformers) (0.16.2)\n",
      "Requirement already satisfied: numpy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from sentence-transformers) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.9/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.9/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in ./.local/lib/python3.9/site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in ./.local/lib/python3.9/site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.12.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.9.2)\n",
      "Requirement already satisfied: requests in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in ./.local/lib/python3.9/site-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in ./.local/lib/python3.9/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers) (12.3.101)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.1)\n",
      "Requirement already satisfied: click in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./.local/lib/python3.9/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.9/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from torchvision->sentence-transformers) (10.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.9/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.9/site-packages (4.36.1)\n",
      "Requirement already satisfied: filelock in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from transformers) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.9/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.local/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./.local/lib/python3.9/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.9.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->transformers) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->transformers) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /pfs/data5/software_uc2/bwhpc/common/jupyter/tensorflow/2023-10-10/lib/python3.9/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install pinecone-client\n",
    "! pip install faunadb\n",
    "! pip install ndg-httpsclient\n",
    "! pip install pyopenssl\n",
    "! pip install pyasn1\n",
    "! pip install -U sentence-transformers\n",
    "! pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PrMFAV259gNf"
   },
   "source": [
    "\n",
    "<h1>Extract Data from .csv</h1>\n",
    "*   AB = Column that corresponds to the text of the abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "UKs6pKMBBwWK"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from itertools import islice\n",
    "import uuid\n",
    "\n",
    "# Load data, store abstract text\n",
    "batch_input = []\n",
    "with open('data/INLPT_class/articles.csv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        batch_input.append(row[\"AB\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KUv3RroBDykt",
    "outputId": "1aee9106-f709-4c2a-ca22-c2950586a140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57560\n"
     ]
    }
   ],
   "source": [
    "print(len(batch_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OpNR2_Rh-W3M"
   },
   "source": [
    "<h3>PubMedBert is a on pubmed data finetuned BERT sentence embedding model. We embedd per Abstract</h3> \n",
    "*   https://huggingface.co/pritamdeka/S-PubMedBert-MS-MARCO\n",
    "\n",
    "<h3>E5-large-V2 a general-purpose embedding model for any tasks requiring a single-vector representation of texts such as retrieval, clustering, and classification   </h3>\n",
    "*   https://huggingface.co/intfloat/e5-large-v22\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fn_vZZmE-Fi7",
    "outputId": "e1a0897d-8f93-4112-add6-00f0f549a10b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 1024, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ")\n",
      "BertTokenizerFast(name_or_path='intfloat/e5-large-v2', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer\n",
    "import re\n",
    "\n",
    "#embed data\n",
    "# model = SentenceTransformer('pritamdeka/S-PubMedBert-MS-MARCO')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('pritamdeka/S-PubMedBert-MS-MARCO')\n",
    "# max_token_size = 350\n",
    "# print(model)\n",
    "# print(tokenizer)\n",
    "\n",
    "model = SentenceTransformer('intfloat/e5-large-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-large-v2')\n",
    "max_token_size = 512\n",
    "print(model)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Chunk data</h3>\n",
    "\n",
    "- Divide each abstract into chunks such that each chunk corresponds to max 512 tokens\n",
    "- Create overlaps between chunks\n",
    "- Add local context as metadata to each chunke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "RFWP7ixvhE6I"
   },
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "def get_front_context(s, m,overlap):\n",
    "  offset = len(overlap.split())\n",
    "  words = s.split()[:-offset]\n",
    "  output = []\n",
    "  count = 0\n",
    "  for word in reversed(words):\n",
    "    if(count > m):\n",
    "      break;\n",
    "    output.insert(0, word)\n",
    "    count += 1\n",
    "  return \" \".join(output)\n",
    "\n",
    "def get_back_context(s, m,overlap):\n",
    "  offset = len(overlap.split())\n",
    "  words = s.split()[offset:]\n",
    "  output = []\n",
    "  count = 0\n",
    "  for word in words:\n",
    "    if(count > m):\n",
    "      break;\n",
    "    output.append(word)\n",
    "    count += 1\n",
    "  return \" \".join(output)\n",
    "\n",
    "def combine_strings(string_1, string_2, n):\n",
    "    words_1 = string_1.split()\n",
    "    words_2 = string_2.split()\n",
    "    front = []\n",
    "    back = []\n",
    "    count = 0\n",
    "\n",
    "    for word in reversed(words_1):\n",
    "        token = tokenizer.convert_ids_to_tokens( tokenizer( word,add_special_tokens=False)[\"input_ids\"])\n",
    "        count += len(token)\n",
    "        if(count > n):\n",
    "            count = 0\n",
    "            break;\n",
    "        front.insert(0, word)\n",
    "\n",
    "    for word in words_2:\n",
    "        token = tokenizer.convert_ids_to_tokens( tokenizer( word,add_special_tokens=False)[\"input_ids\"])\n",
    "        count += len(token)\n",
    "        if(count > n):\n",
    "            count = 0\n",
    "            break;\n",
    "        back.append(word)\n",
    "\n",
    "    front = \" \".join(front)\n",
    "    back = \" \".join(back)\n",
    "    combined_string = front + \" \" + back\n",
    "    return (combined_string, front, back)\n",
    "\n",
    "def refine_sentences_fixed(s, sentences):\n",
    "    all_sentences = []\n",
    "    count = 0\n",
    "    words = sentences.split()\n",
    "    for word in words:\n",
    "        if(count % s == 0):\n",
    "            all_sentences.append([])\n",
    "            count = 0\n",
    "        token = tokenizer.convert_ids_to_tokens( tokenizer(word,add_special_tokens=False)[\"input_ids\"])\n",
    "        if(count + len(token) > s):\n",
    "            all_sentences.append([])\n",
    "            count = 0\n",
    "        count += len(token)\n",
    "        if not all_sentences[len(all_sentences)-1]:\n",
    "            all_sentences[len(all_sentences)-1].append(word)\n",
    "        else:\n",
    "            all_sentences[len(all_sentences)-1][0] +=  \" \" + word\n",
    "\n",
    "    return [item for sublist in all_sentences for item in sublist]\n",
    "\n",
    "def chop_text_by_words(text, n):\n",
    "    m = int(n*0.2)\n",
    "    token_size = max_token_size\n",
    "    dataForm = []\n",
    "    #chop text by tokens\n",
    "    text_chopped_by_words = refine_sentences_fixed(token_size, text)\n",
    "    #make list of tuples for medata and add overlaps\n",
    "    for i, chunk in enumerate(text_chopped_by_words):\n",
    "      if(len(text_chopped_by_words) == 1):\n",
    "        dataForm.append([\"query: \" + chunk, {\"front_context\": \"\"}, {\"back_context\":\"\"},str(uuid.uuid4()), text])\n",
    "        break;\n",
    "      if i == 0:\n",
    "          dataForm.append([\"query: \" + chunk, {\"front_context\": \"\"} , {\"back_context\": \" \".join(text_chopped_by_words[i+1].split()[:m])},str(uuid.uuid4()), text])\n",
    "          overlap = combine_strings(text_chopped_by_words[i],text_chopped_by_words[i+1], token_size/2)\n",
    "          dataForm.append([\"query: \" + overlap[0],{\"front_context\": get_front_context(str(text_chopped_by_words[i]),m,overlap[1])},{\"back_context\":get_back_context(text_chopped_by_words[i+1],m,overlap[2])},str(uuid.uuid4()), text])\n",
    "\n",
    "      elif i == len(text_chopped_by_words)-1:\n",
    "          dataForm.append([\"query: \" + chunk, {\"front_context\": \" \".join(text_chopped_by_words[i-1].split()[-(m):])}, {\"back_context\":\"\"},str(uuid.uuid4()), text])\n",
    "\n",
    "      else:\n",
    "          dataForm.append([\"query: \" + chunk, {\"front_context\": \" \".join(text_chopped_by_words[i-1].split()[-(m):])}, {\"back_context\": \" \".join(text_chopped_by_words[i+1].split()[:(m)])},str(uuid.uuid4()), text ])\n",
    "          overlap = combine_strings(text_chopped_by_words[i],text_chopped_by_words[i+1], token_size/2)\n",
    "          dataForm.append([\"query: \" + overlap[0],  {\"front_context\": get_front_context(str(text_chopped_by_words[i]),m,overlap[1])},{\"back_context\": get_back_context(text_chopped_by_words[i+1],m,overlap[2])},str(uuid.uuid4()), text])\n",
    "\n",
    "    return dataForm\n",
    "\n",
    "def remove_unicode_escape(s):\n",
    "    return re.sub(r'\\\\u....', '', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yuyc83XdiQxj",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16c6498ead84f0aa370e7967b3cf8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "chunks = []\n",
    "for abstract in tqdm(batch_input):\n",
    "  chunks.append(chop_text_by_words(abstract, max_token_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Embedd data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "01svN0PEVTMc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c55b8d47e5842789d5077610bd19340",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "embedding_list = []\n",
    "\n",
    "#This approach creates an embedding for every single chunk of an abstract (or one embedding for abstract if abstract didnt got chunked)\n",
    "all_abstract_chunks = [inner_list[0] for middle_list in chunks for inner_list in middle_list]\n",
    "multi_list = [embedding_list.append(model.encode(chunk)) for chunk in tqdm(all_abstract_chunks)]\n",
    "\n",
    "#This way of embedding results in one embedding per abstract <-> chunked abstracts will be merged together by mean-pooling, this makes no use out of the local context\n",
    "# for batch in tqdm(chunks):\n",
    "#   if(len(batch) == 1):\n",
    "#       embedding_list.append(model.encode(batch[0][0]))\n",
    "#   elif len(batch) > 1:\n",
    "#       flattened_list = [level_2 for level_1 in batch for level_2 in level_1[0:1] if level_1]\n",
    "#       multi_list = [model.encode(chunk) for chunk in flattened_list]\n",
    "#       embedding_list.append(np.mean(multi_list, axis=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   batch_input: List of abstracts\n",
    "-   chunks: List of lists of lists of chunked abstracts + local context for metadata +id + text of whole abstract\n",
    "-   embedding_list: list of embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w_MWkqLF_Cbb"
   },
   "source": [
    "Create a vector that holds the data that is going to be uploaded to Pinecone Vector DB. To Pinecone we upload the embedding together with an id that maps the embedding to its original text in the DB (FaunaDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prepare data, initialize pinecone manager and upload</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "0g5muWkFAw4r",
    "outputId": "1be09a1c-d0b6-4e8c-c1c9-d95f4232f8a2",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e1d19c14bf40b89a81c53763943e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "pinecone_vectors = []\n",
    "count = 0\n",
    "for batch in tqdm(chunks):\n",
    "    for data in batch:\n",
    "        pinecone_vectors.append((data[3], embedding_list[count].tolist()))\n",
    "        count += 1\n",
    "        \n",
    "#createvec = [{pinecone_vectors.append((data[3], embedding_list[count])),}  for batch in tqdm(chunks) for data in batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fOS8BZSwAk8T",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "#Init pinecone index\n",
    "pinecone.init(api_key=\"4d2c2cd0-cf55-43c5-afb1-11001dc68709\", environment=\"gcp-starter\")\n",
    "index = pinecone.Index(\"inlp-med-ws2324\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WRUXD4AzqOy9"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import itertools\n",
    "\n",
    "#Split data into m n-sized lists, used for bulk upload\n",
    "def bulk_upload(iterable, n):\n",
    "  bulk = [iterable[x:x+n] for x in range(0, len(iterable), n)]\n",
    "  return bulk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 210
    },
    "id": "rlIADzUeFjMx",
    "outputId": "b1c51a0c-589a-4dab-b412-5eb15993f4dc",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93dc0c8f2f414cac99528788b16255c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#upsert data to pinecone\n",
    "for ids_vectors_chunk in tqdm(bulk_upload(pinecone_vectors,500)):\n",
    "  index.upsert(vectors=ids_vectors_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNRn93foCBzO"
   },
   "source": [
    "<h3>Prepare Data, initialize FaunaDB client and upload</h3>\n",
    "\n",
    "*   https://v4.dashboard.fauna.com/db/eu/medicalData\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dHGE7Qhah-w3"
   },
   "outputs": [],
   "source": [
    "#Initialize FaunaDB\n",
    "from faunadb import query as q\n",
    "from faunadb.objects import Ref\n",
    "from faunadb.client import FaunaClient\n",
    "\n",
    "client = FaunaClient(\n",
    "  secret=\"fnAFW8NnOqAAzXyJUw9gkBUoCOopcrX3c8zdIJy0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "pT_UhYM5CJi-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67080\n"
     ]
    }
   ],
   "source": [
    "chunk_test = chunks\n",
    "upload = []\n",
    "for chunk in chunk_test:\n",
    "    for data in chunk:\n",
    "        upload.append(data)\n",
    "            \n",
    "print(len(upload))\n",
    "chunkeddata = bulk_upload(upload, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "3yjIPoVzpLYR"
   },
   "outputs": [],
   "source": [
    "#Upload data into FaunaDB\n",
    "def upload_data_to_fauna(data):\n",
    "        for row in data:\n",
    "            client.query(q.create(q.collection(\"metadata\"),{\"data\": {\"chunk\": row[0], \"front_context\": row[1],\"back_context\": row[2],\"id\": row[3],\"abstract\": row[4]}}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nsJYeVI5A9w"
   },
   "outputs": [],
   "source": [
    "for i in range(len(chunkeddata)):\n",
    "  upload_data_to_fauna(chunkeddata[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwyyWG-qOtRL"
   },
   "source": [
    "How to query from Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uKpqgFe_Toxf",
    "outputId": "42db386b-13aa-4b75-98a9-cc97872adb72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': '65639466-3a21-4e2d-8c91-6fc106523e02',\n",
       "              'score': 0.850698709,\n",
       "              'values': []},\n",
       "             {'id': 'ef02e706-a21f-44c7-94f4-d79f1969b22f',\n",
       "              'score': 0.844490886,\n",
       "              'values': []},\n",
       "             {'id': '7e9e6077-1f25-4b27-abb4-b477a17144b2',\n",
       "              'score': 0.835636497,\n",
       "              'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def return_document(query):\n",
    "  embeded_vector = model.encode(query).tolist()\n",
    "\n",
    "  query_response = index.query(\n",
    "      embeded_vector,\n",
    "      top_k=3,\n",
    "      )\n",
    "  return query_response\n",
    "\n",
    "return_document(\"CASK disorder phenotype\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npoqkqNnPCu1"
   },
   "source": [
    "How to Query from FaunaDB, first create index then query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2j-PrNpmA1T3",
    "outputId": "0efa5ab0-00f4-45db-ced2-a9953f2d530d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [['query: CLINICAL CHARACTERISTICS: CASK disorders include a spectrum of phenotypes in both females and males. Two main types of clinical presentation are seen: Microcephalywith pontine and cerebellar hypoplasia (MICPCH), generally associated withpathogenic loss-of-function variants in CASK. X-linked intellectual disability(XLID) with or without nystagmus, generally associated with hypomorphic CASKpathogenic variants. MICPCH is typically seen in females with moderate-to-severeintellectual disability, progressive microcephaly with or without ophthalmologicanomalies, and sensorineural hearing loss. Most are able to sit independently;20%-25% attain the ability to walk; language is nearly absent in most. Neurologicfeatures may include axial hypotonia, hypertonia/spasticity of the extremities,and dystonia or other movement disorders. Nearly 40% have seizures by age tenyears. Behaviors may include sleep disturbances, hand stereotypies, and selfbiting. MICPCH in males may occur with or without severe epileptic encephalopathyin addition to severe-to-profound developmental delay. When seizures are presentthey occur early and may be intractable. In individuals and families with milder(i.e., hypomorphic) pathogenic variants, the clinical phenotype is usually thatof XLID with or without nystagmus and additional clinical features. Males havemild-to-severe intellectual disability, with or without nystagmus and otherocular features. Females typically have normal intelligence with some displayingmild-to-severe intellectual disability with or without ocular features.DIAGNOSIS/TESTING: The diagnosis of a CASK disorder is established in a femalewho is heterozygous for a CASK pathogenic variant and in a male who is hemizygousfor a CASK pathogenic variant on molecular genetic testing. Rarely, affectedmales have a mosaic pathogenic variant. MANAGEMENT: Treatment of manifestations:Treatment is symptomatic and includes standard management of developmental delayand intellectual disability issues; medication for seizures; nutritional support;use of physiotherapy; and treatment of abnormal vision or hearing loss. GENETICCOUNSELING: CASK disorders are inherited in an X-linked manner. Risk to thefamily members of a proband with a', None, None, 'CLINICAL CHARACTERISTICS: CASK disorders include a spectrum of phenotypes in both females and males. Two main types of clinical presentation are seen: Microcephalywith pontine and cerebellar hypoplasia (MICPCH), generally associated withpathogenic loss-of-function variants in CASK. X-linked intellectual disability(XLID) with or without nystagmus, generally associated with hypomorphic CASKpathogenic variants. MICPCH is typically seen in females with moderate-to-severeintellectual disability, progressive microcephaly with or without ophthalmologicanomalies, and sensorineural hearing loss. Most are able to sit independently;20%-25% attain the ability to walk; language is nearly absent in most. Neurologicfeatures may include axial hypotonia, hypertonia/spasticity of the extremities,and dystonia or other movement disorders. Nearly 40% have seizures by age tenyears. Behaviors may include sleep disturbances, hand stereotypies, and selfbiting. MICPCH in males may occur with or without severe epileptic encephalopathyin addition to severe-to-profound developmental delay. When seizures are presentthey occur early and may be intractable. In individuals and families with milder(i.e., hypomorphic) pathogenic variants, the clinical phenotype is usually thatof XLID with or without nystagmus and additional clinical features. Males havemild-to-severe intellectual disability, with or without nystagmus and otherocular features. Females typically have normal intelligence with some displayingmild-to-severe intellectual disability with or without ocular features.DIAGNOSIS/TESTING: The diagnosis of a CASK disorder is established in a femalewho is heterozygous for a CASK pathogenic variant and in a male who is hemizygousfor a CASK pathogenic variant on molecular genetic testing. Rarely, affectedmales have a mosaic pathogenic variant. MANAGEMENT: Treatment of manifestations:Treatment is symptomatic and includes standard management of developmental delayand intellectual disability issues; medication for seizures; nutritional support;use of physiotherapy; and treatment of abnormal vision or hearing loss. GENETICCOUNSELING: CASK disorders are inherited in an X-linked manner. Risk to thefamily members of a proband with a CASK disorder depends on the phenotype (i.e.,MICPCH or XLID +/- nystagmus) in the proband. MICPCH. Most affected females andmales represent simplex cases (i.e., the only affected family member) and havethe disorder as the result of a de novo CASK pathogenic variant. Becauseheterozygous females manifest the phenotype, an asymptomatic mother is unlikelyto be heterozygous for the CASK pathogenic variant. If a proband represents asimplex case, the recurrence risk to sibs appears to be low but greater than thatof the general population because of the possibility of parental germlinemosaicism. XLID +/- nystagmus. The father of a male with a CASK disorder will nothave the disorder nor will he be hemizygous for the CASK pathogenic variant. If amale is the only affected family member, the mother may be a heterozygote or theaffected male may have a de novo pathogenic variant. In a family with more thanone affected individual, the mother of an affected male is an obligateheterozygote. If the mother of the proband has a CASK pathogenic variant, thechance of transmitting it in each pregnancy is 50%: males who inherit thepathogenic variant will be affected; females who inherit the pathogenic variantwill typically be asymptomatic but may have a range of manifestations. If theCASK pathogenic variant cannot be detected in maternal leukocyte DNA, the risk tosibs is greater than that of the general population because of the possibility ofparental germline mosaicism. Once the CASK pathogenic variant has been identifiedin an affected family member, prenatal testing for a pregnancy at increased riskand preimplantation genetic testing for a CASK disorder are possible.', '65639466-3a21-4e2d-8c91-6fc106523e02']]}\n"
     ]
    }
   ],
   "source": [
    "result = client.query(\n",
    "  q.paginate(q.match(q.index(\"metadata\"), \"65639466-3a21-4e2d-8c91-6fc106523e02\"))\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2FAl_RoPF6C"
   },
   "source": [
    "ToDo: Implement Pinecone and FaunaDB API into backend\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
